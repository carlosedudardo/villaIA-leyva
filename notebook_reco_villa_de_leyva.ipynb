{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dc148e",
   "metadata": {},
   "source": [
    "# üìí Notebook: Recomendador Villa de Leyva ‚Äî entrenamiento, evaluaci√≥n y simulaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890d16c",
   "metadata": {},
   "source": [
    "## 0) Configuraci√≥n inicial y utilidades (ejecuta primero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3304c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 0) Configuraci√≥n y utilidades\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Rutas y par√°metros ---\n",
    "ENC = \"utf-8-sig\"\n",
    "SEP = \";\"\n",
    "DATA_PATH = \"dataset_Recomendacion_villa_de_leyva_eleccion (2).csv\"\n",
    "CAT_PATH  = \"catalogo_vdl_lugares_unico.csv\"\n",
    "RANDOM_SEED = 42\n",
    "K_LIST = [3, 5, 10]\n",
    "TEST_USER_FRAC = 0.20\n",
    "\n",
    "# --- Normalizador de encabezados con caracteres raros ---\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ren = {\n",
    "        \"compa¬§¬°a_viaje\": \"compania_viaje\",\n",
    "        \"‚Äöpoca_visita\": \"epoca_visita\",\n",
    "        \"compa√±ia_viaje\": \"compania_viaje\",\n",
    "    }\n",
    "    ren = {k: v for k, v in ren.items() if k in df.columns}\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "# --- Canonicalizaci√≥n de valores categ√≥ricos ---\n",
    "CANON_MAP = {\n",
    "    \"gastronomia\": \"gastronomico\",\n",
    "    \"gastronom√≠a\": \"gastronomico\",\n",
    "    \"relax_fotografia\": \"relax_fotografia\",\n",
    "    \"relax_fotograf√≠a\": \"relax_fotografia\",\n",
    "    \"parque tematico\": \"parque_tematico\",\n",
    "    \"enoturismo\": \"gastronomico\",\n",
    "    \"historico\": \"centro_historico\",\n",
    "    \"hist√≥rico\": \"centro_historico\",\n",
    "    \"natural\": \"naturaleza\",\n",
    "    # a√±adidos √∫tiles:\n",
    "    \"cultura\": \"cultural\",\n",
    "    \"historia\": \"cultural\",\n",
    "    \"fotografia\": \"relax_fotografia\",\n",
    "}\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = s.strip().lower().replace(\" \", \"_\")\n",
    "    return CANON_MAP.get(s, s)\n",
    "\n",
    "def normalize_values(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df_in.copy()\n",
    "    # (ajustado a columnas existentes)\n",
    "    cols = [\"tipo_turista_preferido\", \"tipo_sitio\", \"epoca_visita\",\n",
    "            \"accesibilidad_general\", \"ubicacion_geografica\"]\n",
    "    for c in cols:\n",
    "        if c in df2.columns:\n",
    "            df2[c] = df2[c].astype(str).map(canon)\n",
    "    return df2\n",
    "\n",
    "# --- Columnas base (ajustadas al CSV actual) ---\n",
    "CAT_COLS = [\n",
    "    \"nacionalidad\",\"origen\",\"tipo_turista_preferido\",\"compania_viaje\",\n",
    "    \"nombre_sitio\",\"tipo_sitio\",\"accesibilidad_general\",\n",
    "    \"ubicacion_geografica\",\"epoca_visita\",\"restricciones_movilidad\",\"idioma_info\",\"clima_predominante\"\n",
    "]\n",
    "NUM_COLS = [\n",
    "    \"edad\",\"presupuesto_estimado\",\"costo_entrada\",\"admite_mascotas\",\"frecuencia_viaje\",\"sitios_visitados\",\"calificacion_sitios_previos\",\"tiempo_estancia_promedio\",\"afluencia_promedio\",\"duracion_esperada\"\n",
    "]\n",
    "\n",
    "# --- Afinidad perfil √ó tipo (can√≥nica) ---\n",
    "AFINIDAD = {\n",
    "    \"cultural\":   {\"museo\":0.9,\"centro_historico\":0.9,\"religioso\":0.7,\"arquitectura\":0.85,\"museo_religioso\":0.8,\"arqueologico\":0.85,\"plaza\":0.7},\n",
    "    \"naturaleza\": {\"naturaleza\":0.95,\"senderismo\":0.9,\"mirador\":0.8,\"parque_urbano\":0.6},\n",
    "    \"aventura\":   {\"aventura\":0.95,\"senderismo\":0.85,\"parque_tematico\":0.7},\n",
    "    \"gastronomico\":{\"gastronomico\":0.95,\"enoturismo\":0.9,\"artesanal\":0.6,\"plaza\":0.6},\n",
    "    \"relax_fotografia\": {\"mirador\":0.9,\"plaza\":0.8,\"arquitectura\":0.8,\"naturaleza\":0.75},\n",
    "}\n",
    "\n",
    "# --- Feature engineering ---\n",
    "def make_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # evita divisi√≥n por cero y NaN\n",
    "    presu = pd.to_numeric(X.get(\"presupuesto_estimado\", 0), errors=\"coerce\")\n",
    "    costo = pd.to_numeric(X.get(\"costo_entrada\", 0), errors=\"coerce\")\n",
    "    denom = (presu * 0.15).replace(0, np.nan)\n",
    "    X[\"ratio_costo_presu\"] = (costo / denom).clip(0, 3).fillna(0)\n",
    "\n",
    "    def _afinidad_row(r):\n",
    "        return AFINIDAD.get(str(r.get(\"tipo_turista_preferido\",\"\")), {})\\\n",
    "                       .get(str(r.get(\"tipo_sitio\",\"\")), 0.5)\n",
    "\n",
    "    X[\"afinidad_tipo\"] = X.apply(_afinidad_row, axis=1)\n",
    "    X[\"x_tipoTur__tipoSit\"] = X.get(\"tipo_turista_preferido\",\"\").astype(str) + \"√ó\" + X.get(\"tipo_sitio\",\"\").astype(str)\n",
    "    X[\"x_epoca__tipoSit\"]   = X.get(\"epoca_visita\",\"\").astype(str) + \"√ó\" + X.get(\"tipo_sitio\",\"\").astype(str)\n",
    "    return X\n",
    "\n",
    "# --- Columnas extendidas para modelado ---\n",
    "CAT_COLS_X = CAT_COLS + [\"x_tipoTur__tipoSit\",\"x_epoca__tipoSit\"]\n",
    "NUM_COLS_X = NUM_COLS + [\"ratio_costo_presu\",\"afinidad_tipo\"]\n",
    "\n",
    "# --- Helpers anti-fugas / duplicados (√∫tiles m√°s adelante) ---\n",
    "LEAK_COLS = [\"y_like\",\"rating_usuario\",\"sitio_recomendado\"]\n",
    "\n",
    "def drop_leaks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop(columns=[c for c in LEAK_COLS if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "def dedupe_columns(df: pd.DataFrame, keep=\"last\") -> pd.DataFrame:\n",
    "    \"\"\"Elimina columnas duplicadas por nombre (√∫til tras concat usuario√ócat√°logo).\"\"\"\n",
    "    return df.loc[:, ~df.columns.duplicated(keep=keep)].copy()\n",
    "\n",
    "# --- M√©tricas Top-K ---\n",
    "def _dcg_at_k(rels):\n",
    "    return float(np.sum([r/np.log2(i+2) for i, r in enumerate(rels)]))\n",
    "\n",
    "def recall_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False)\n",
    "    topk = g.head(k)\n",
    "    tot = g[rel_col].sum()\n",
    "    return float(\"nan\") if tot==0 else float(topk[rel_col].sum()/tot)\n",
    "\n",
    "def ndcg_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False)\n",
    "    dcg  = _dcg_at_k(g.head(k)[rel_col].tolist())\n",
    "    idcg = _dcg_at_k(sorted(g[rel_col].tolist(), reverse=True)[:k])\n",
    "    return float(\"nan\") if idcg==0 else float(dcg/idcg)\n",
    "\n",
    "def coverage_at_k(df, k, score_col, item_col=\"nombre_sitio\"):\n",
    "    topk = (df.sort_values([\"id_usuario\", score_col], ascending=[True, False])\n",
    "              .groupby(\"id_usuario\").head(k))\n",
    "    return float(topk[item_col].nunique() / df[item_col].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a9727",
   "metadata": {},
   "source": [
    "## 1) Carga, limpieza, features y etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c301f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100000, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_usuario</th>\n",
       "      <th>edad</th>\n",
       "      <th>nacionalidad</th>\n",
       "      <th>origen</th>\n",
       "      <th>tipo_turista_preferido</th>\n",
       "      <th>compania_viaje</th>\n",
       "      <th>frecuencia_viaje</th>\n",
       "      <th>restricciones_movilidad</th>\n",
       "      <th>presupuesto_estimado</th>\n",
       "      <th>sitios_visitados</th>\n",
       "      <th>...</th>\n",
       "      <th>idioma_info</th>\n",
       "      <th>ubicacion_geografica</th>\n",
       "      <th>clima_predominante</th>\n",
       "      <th>epoca_visita</th>\n",
       "      <th>rating_usuario</th>\n",
       "      <th>ratio_costo_presu</th>\n",
       "      <th>afinidad_tipo</th>\n",
       "      <th>x_tipoTur__tipoSit</th>\n",
       "      <th>x_epoca__tipoSit</th>\n",
       "      <th>y_like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U03462</td>\n",
       "      <td>40</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Duitama</td>\n",
       "      <td>gastronomico</td>\n",
       "      <td>familia</td>\n",
       "      <td>2</td>\n",
       "      <td>ninguna</td>\n",
       "      <td>572831</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>es</td>\n",
       "      <td>raquira</td>\n",
       "      <td>seco</td>\n",
       "      <td>fin_de_semana</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.081467</td>\n",
       "      <td>0.95</td>\n",
       "      <td>gastronomico√ógastronomico</td>\n",
       "      <td>fin_de_semana√ógastronomico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U13281</td>\n",
       "      <td>31</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Cali</td>\n",
       "      <td>naturaleza</td>\n",
       "      <td>solo</td>\n",
       "      <td>1</td>\n",
       "      <td>ninguna</td>\n",
       "      <td>191317</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>es</td>\n",
       "      <td>gachantiva</td>\n",
       "      <td>templado_humedo</td>\n",
       "      <td>fin_de_semana</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.420593</td>\n",
       "      <td>0.50</td>\n",
       "      <td>naturaleza√óparque_tematico</td>\n",
       "      <td>fin_de_semana√óparque_tematico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U12443</td>\n",
       "      <td>16</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>C√∫cuta</td>\n",
       "      <td>gastronomico</td>\n",
       "      <td>solo</td>\n",
       "      <td>2</td>\n",
       "      <td>ninguna</td>\n",
       "      <td>172745</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>es/en</td>\n",
       "      <td>villa_de_leyva</td>\n",
       "      <td>templado_seco</td>\n",
       "      <td>temporada_baja</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.077185</td>\n",
       "      <td>0.50</td>\n",
       "      <td>gastronomico√ócentro_historico</td>\n",
       "      <td>temporada_baja√ócentro_historico</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_usuario  edad nacionalidad   origen tipo_turista_preferido  \\\n",
       "0     U03462    40     Colombia  Duitama           gastronomico   \n",
       "1     U13281    31     Colombia     Cali             naturaleza   \n",
       "2     U12443    16     Colombia   C√∫cuta           gastronomico   \n",
       "\n",
       "  compania_viaje  frecuencia_viaje restricciones_movilidad  \\\n",
       "0        familia                 2                 ninguna   \n",
       "1           solo                 1                 ninguna   \n",
       "2           solo                 2                 ninguna   \n",
       "\n",
       "   presupuesto_estimado  sitios_visitados  ...  idioma_info  \\\n",
       "0                572831                 8  ...           es   \n",
       "1                191317                 5  ...           es   \n",
       "2                172745                21  ...        es/en   \n",
       "\n",
       "   ubicacion_geografica clima_predominante    epoca_visita  rating_usuario  \\\n",
       "0               raquira               seco   fin_de_semana             4.4   \n",
       "1            gachantiva    templado_humedo   fin_de_semana             4.0   \n",
       "2        villa_de_leyva      templado_seco  temporada_baja             2.5   \n",
       "\n",
       "   ratio_costo_presu  afinidad_tipo             x_tipoTur__tipoSit  \\\n",
       "0           0.081467           0.95      gastronomico√ógastronomico   \n",
       "1           0.420593           0.50     naturaleza√óparque_tematico   \n",
       "2           0.077185           0.50  gastronomico√ócentro_historico   \n",
       "\n",
       "                  x_epoca__tipoSit y_like  \n",
       "0       fin_de_semana√ógastronomico      1  \n",
       "1    fin_de_semana√óparque_tematico      1  \n",
       "2  temporada_baja√ócentro_historico      0  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1) Carga, limpieza y features\n",
    "# ===============================\n",
    "df_raw = pd.read_csv(DATA_PATH, sep=SEP, encoding=ENC)\n",
    "df_raw = normalize_columns(df_raw)\n",
    "df_raw.columns = df_raw.columns.str.strip()\n",
    "\n",
    "# Canonicaliza valores categ√≥ricos clave\n",
    "df_raw = normalize_values(df_raw)\n",
    "\n",
    "# Coerci√≥n num√©rica (solo si existen)\n",
    "for c in [\"costo_entrada\",\"presupuesto_estimado\",\"edad\",\"frecuencia_viaje\",\n",
    "          \"sitios_visitados\",\"calificacion_sitios_previos\",\"tiempo_estancia_promedio\",\n",
    "          \"afluencia_promedio\",\"duracion_esperada\",\"admite_mascotas\",\"rating_usuario\"]:\n",
    "    if c in df_raw.columns:\n",
    "        df_raw[c] = pd.to_numeric(df_raw[c], errors=\"coerce\")\n",
    "\n",
    "# Anti-leakage obvio\n",
    "df_raw = df_raw.drop(columns=[\"sitio_recomendado\"], errors=\"ignore\")\n",
    "\n",
    "# Elimina posibles columnas duplicadas por nombre\n",
    "df_raw = dedupe_columns(df_raw, keep=\"last\")\n",
    "\n",
    "# Validaci√≥n m√≠nima antes de features (lo que make_features y el modelo necesitan)\n",
    "REQUIRED = [\"tipo_turista_preferido\",\"tipo_sitio\",\"epoca_visita\",\n",
    "            \"presupuesto_estimado\",\"costo_entrada\",\"nombre_sitio\",\"id_usuario\"]\n",
    "missing = [c for c in REQUIRED if c not in df_raw.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Faltan columnas obligatorias: {missing}\")\n",
    "\n",
    "# Features\n",
    "df_feat = make_features(df_raw)\n",
    "\n",
    "# Etiqueta binaria (mant√©n rating_usuario como referencia si lo tienes)\n",
    "if \"rating_usuario\" not in df_feat.columns:\n",
    "    raise KeyError(\"Falta 'rating_usuario' para crear y_like.\")\n",
    "df_feat[\"y_like\"] = (df_feat[\"rating_usuario\"] >= 4.0).astype(int)\n",
    "\n",
    "# Mant√©n id_usuario como string para el GroupKFold (no lo uses como feature)\n",
    "df_feat[\"id_usuario\"] = df_feat[\"id_usuario\"].astype(\"string\")\n",
    "\n",
    "print(\"Shape:\", df_feat.shape)\n",
    "df_feat.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10fe69",
   "metadata": {},
   "source": [
    "## 2) Split honesto por usuario (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d7fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios train/test: 16000 / 4000\n",
      "Filas train/test: (80004, 29) (19996, 29)\n",
      "Positivos (train/test): 0.3326 / 0.3234\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2) Split por usuario (sin fuga de informaci√≥n)\n",
    "# ================================================\n",
    "base = df_feat.copy()\n",
    "\n",
    "# Asegura id_usuario usable\n",
    "base[\"id_usuario\"] = base[\"id_usuario\"].astype(\"string\")\n",
    "base = base[~base[\"id_usuario\"].isna()]\n",
    "\n",
    "# Conjunto de usuarios y muestreo reproducible\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "users = base[\"id_usuario\"].drop_duplicates().to_numpy()\n",
    "n_test = max(1, int(round(len(users) * TEST_USER_FRAC)))\n",
    "test_users = set(rng.choice(users, size=n_test, replace=False))\n",
    "\n",
    "# Particiones por usuario (hold-out honesto)\n",
    "train_df = base[~base[\"id_usuario\"].isin(test_users)].reset_index(drop=True)\n",
    "test_df  = base[ base[\"id_usuario\"].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(\"Usuarios train/test:\", train_df[\"id_usuario\"].nunique(), \"/\", test_df[\"id_usuario\"].nunique())\n",
    "print(\"Filas train/test:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# No debe haber fuga de usuarios\n",
    "overlap = set(train_df[\"id_usuario\"]).intersection(set(test_df[\"id_usuario\"]))\n",
    "assert len(overlap) == 0, f\"Fuga de usuarios entre splits: {len(overlap)}\"\n",
    "\n",
    "# (Opcional) sanity de la etiqueta por split\n",
    "if \"y_like\" in train_df.columns:\n",
    "    print(\"Positivos (train/test):\",\n",
    "          round(train_df[\"y_like\"].mean(), 4),\n",
    "          \"/\",\n",
    "          round(test_df[\"y_like\"].mean(), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d05bc",
   "metadata": {},
   "source": [
    "## 3) Entrenamiento con PyCaret (compare ‚Üí tune ‚Üí blend ‚Üí save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b9420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0f81c th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0f81c_row0_col0, #T_0f81c_row0_col3, #T_0f81c_row0_col4, #T_0f81c_row0_col5, #T_0f81c_row1_col0, #T_0f81c_row1_col1, #T_0f81c_row1_col2, #T_0f81c_row1_col3, #T_0f81c_row1_col4, #T_0f81c_row1_col6, #T_0f81c_row1_col7, #T_0f81c_row2_col0, #T_0f81c_row2_col1, #T_0f81c_row2_col2, #T_0f81c_row2_col3, #T_0f81c_row2_col4, #T_0f81c_row2_col5, #T_0f81c_row2_col6, #T_0f81c_row2_col7, #T_0f81c_row3_col0, #T_0f81c_row3_col1, #T_0f81c_row3_col2, #T_0f81c_row3_col3, #T_0f81c_row3_col4, #T_0f81c_row3_col5, #T_0f81c_row3_col6, #T_0f81c_row3_col7, #T_0f81c_row4_col0, #T_0f81c_row4_col1, #T_0f81c_row4_col2, #T_0f81c_row4_col3, #T_0f81c_row4_col4, #T_0f81c_row4_col5, #T_0f81c_row4_col6, #T_0f81c_row4_col7, #T_0f81c_row5_col0, #T_0f81c_row5_col1, #T_0f81c_row5_col2, #T_0f81c_row5_col3, #T_0f81c_row5_col4, #T_0f81c_row5_col5, #T_0f81c_row5_col6, #T_0f81c_row5_col7, #T_0f81c_row6_col0, #T_0f81c_row6_col1, #T_0f81c_row6_col2, #T_0f81c_row6_col3, #T_0f81c_row6_col4, #T_0f81c_row6_col5, #T_0f81c_row6_col6, #T_0f81c_row6_col7, #T_0f81c_row7_col0, #T_0f81c_row7_col1, #T_0f81c_row7_col2, #T_0f81c_row7_col3, #T_0f81c_row7_col4, #T_0f81c_row7_col5, #T_0f81c_row7_col6, #T_0f81c_row7_col7, #T_0f81c_row8_col0, #T_0f81c_row8_col1, #T_0f81c_row8_col2, #T_0f81c_row8_col3, #T_0f81c_row8_col5, #T_0f81c_row8_col6, #T_0f81c_row8_col7, #T_0f81c_row9_col0, #T_0f81c_row9_col1, #T_0f81c_row9_col2, #T_0f81c_row9_col3, #T_0f81c_row9_col4, #T_0f81c_row9_col5, #T_0f81c_row9_col6, #T_0f81c_row9_col7, #T_0f81c_row10_col0, #T_0f81c_row10_col1, #T_0f81c_row10_col2, #T_0f81c_row10_col3, #T_0f81c_row10_col4, #T_0f81c_row10_col5, #T_0f81c_row10_col6, #T_0f81c_row10_col7, #T_0f81c_row11_col0, #T_0f81c_row11_col1, #T_0f81c_row11_col2, #T_0f81c_row11_col3, #T_0f81c_row11_col4, #T_0f81c_row11_col5, #T_0f81c_row11_col6, #T_0f81c_row11_col7, #T_0f81c_row12_col0, #T_0f81c_row12_col1, #T_0f81c_row12_col2, #T_0f81c_row12_col3, #T_0f81c_row12_col4, #T_0f81c_row12_col5, #T_0f81c_row12_col6, #T_0f81c_row12_col7, #T_0f81c_row13_col0, #T_0f81c_row13_col1, #T_0f81c_row13_col2, #T_0f81c_row13_col3, #T_0f81c_row13_col4, #T_0f81c_row13_col5, #T_0f81c_row13_col6, #T_0f81c_row13_col7, #T_0f81c_row14_col0, #T_0f81c_row14_col1, #T_0f81c_row14_col2, #T_0f81c_row14_col3, #T_0f81c_row14_col4, #T_0f81c_row14_col5, #T_0f81c_row14_col6, #T_0f81c_row14_col7, #T_0f81c_row15_col0, #T_0f81c_row15_col1, #T_0f81c_row15_col2, #T_0f81c_row15_col4, #T_0f81c_row15_col5, #T_0f81c_row15_col6, #T_0f81c_row15_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0f81c_row0_col1, #T_0f81c_row0_col2, #T_0f81c_row0_col6, #T_0f81c_row0_col7, #T_0f81c_row1_col5, #T_0f81c_row8_col4, #T_0f81c_row15_col3 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_0f81c_row0_col8, #T_0f81c_row2_col8, #T_0f81c_row3_col8, #T_0f81c_row4_col8, #T_0f81c_row5_col8, #T_0f81c_row6_col8, #T_0f81c_row7_col8, #T_0f81c_row8_col8, #T_0f81c_row9_col8, #T_0f81c_row10_col8, #T_0f81c_row11_col8, #T_0f81c_row12_col8, #T_0f81c_row13_col8, #T_0f81c_row14_col8, #T_0f81c_row15_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_0f81c_row1_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0f81c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0f81c_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_0f81c_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_0f81c_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_0f81c_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_0f81c_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_0f81c_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_0f81c_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_0f81c_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_0f81c_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row0\" class=\"row_heading level0 row0\" >gbc</th>\n",
       "      <td id=\"T_0f81c_row0_col0\" class=\"data row0 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_0f81c_row0_col1\" class=\"data row0 col1\" >0.8491</td>\n",
       "      <td id=\"T_0f81c_row0_col2\" class=\"data row0 col2\" >0.9216</td>\n",
       "      <td id=\"T_0f81c_row0_col3\" class=\"data row0 col3\" >0.7550</td>\n",
       "      <td id=\"T_0f81c_row0_col4\" class=\"data row0 col4\" >0.7834</td>\n",
       "      <td id=\"T_0f81c_row0_col5\" class=\"data row0 col5\" >0.7690</td>\n",
       "      <td id=\"T_0f81c_row0_col6\" class=\"data row0 col6\" >0.6569</td>\n",
       "      <td id=\"T_0f81c_row0_col7\" class=\"data row0 col7\" >0.6572</td>\n",
       "      <td id=\"T_0f81c_row0_col8\" class=\"data row0 col8\" >17.5250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row1\" class=\"row_heading level0 row1\" >ridge</th>\n",
       "      <td id=\"T_0f81c_row1_col0\" class=\"data row1 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_0f81c_row1_col1\" class=\"data row1 col1\" >0.8297</td>\n",
       "      <td id=\"T_0f81c_row1_col2\" class=\"data row1 col2\" >0.9210</td>\n",
       "      <td id=\"T_0f81c_row1_col3\" class=\"data row1 col3\" >0.8539</td>\n",
       "      <td id=\"T_0f81c_row1_col4\" class=\"data row1 col4\" >0.7001</td>\n",
       "      <td id=\"T_0f81c_row1_col5\" class=\"data row1 col5\" >0.7694</td>\n",
       "      <td id=\"T_0f81c_row1_col6\" class=\"data row1 col6\" >0.6365</td>\n",
       "      <td id=\"T_0f81c_row1_col7\" class=\"data row1 col7\" >0.6444</td>\n",
       "      <td id=\"T_0f81c_row1_col8\" class=\"data row1 col8\" >2.9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row2\" class=\"row_heading level0 row2\" >lda</th>\n",
       "      <td id=\"T_0f81c_row2_col0\" class=\"data row2 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_0f81c_row2_col1\" class=\"data row2 col1\" >0.8295</td>\n",
       "      <td id=\"T_0f81c_row2_col2\" class=\"data row2 col2\" >0.9209</td>\n",
       "      <td id=\"T_0f81c_row2_col3\" class=\"data row2 col3\" >0.8535</td>\n",
       "      <td id=\"T_0f81c_row2_col4\" class=\"data row2 col4\" >0.6999</td>\n",
       "      <td id=\"T_0f81c_row2_col5\" class=\"data row2 col5\" >0.7691</td>\n",
       "      <td id=\"T_0f81c_row2_col6\" class=\"data row2 col6\" >0.6361</td>\n",
       "      <td id=\"T_0f81c_row2_col7\" class=\"data row2 col7\" >0.6439</td>\n",
       "      <td id=\"T_0f81c_row2_col8\" class=\"data row2 col8\" >3.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n",
       "      <td id=\"T_0f81c_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_0f81c_row3_col1\" class=\"data row3 col1\" >0.8467</td>\n",
       "      <td id=\"T_0f81c_row3_col2\" class=\"data row3 col2\" >0.9200</td>\n",
       "      <td id=\"T_0f81c_row3_col3\" class=\"data row3 col3\" >0.7453</td>\n",
       "      <td id=\"T_0f81c_row3_col4\" class=\"data row3 col4\" >0.7833</td>\n",
       "      <td id=\"T_0f81c_row3_col5\" class=\"data row3 col5\" >0.7638</td>\n",
       "      <td id=\"T_0f81c_row3_col6\" class=\"data row3 col6\" >0.6504</td>\n",
       "      <td id=\"T_0f81c_row3_col7\" class=\"data row3 col7\" >0.6509</td>\n",
       "      <td id=\"T_0f81c_row3_col8\" class=\"data row3 col8\" >4.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row4\" class=\"row_heading level0 row4\" >ada</th>\n",
       "      <td id=\"T_0f81c_row4_col0\" class=\"data row4 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_0f81c_row4_col1\" class=\"data row4 col1\" >0.8415</td>\n",
       "      <td id=\"T_0f81c_row4_col2\" class=\"data row4 col2\" >0.9182</td>\n",
       "      <td id=\"T_0f81c_row4_col3\" class=\"data row4 col3\" >0.7932</td>\n",
       "      <td id=\"T_0f81c_row4_col4\" class=\"data row4 col4\" >0.7467</td>\n",
       "      <td id=\"T_0f81c_row4_col5\" class=\"data row4 col5\" >0.7691</td>\n",
       "      <td id=\"T_0f81c_row4_col6\" class=\"data row4 col6\" >0.6487</td>\n",
       "      <td id=\"T_0f81c_row4_col7\" class=\"data row4 col7\" >0.6495</td>\n",
       "      <td id=\"T_0f81c_row4_col8\" class=\"data row4 col8\" >7.2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row5\" class=\"row_heading level0 row5\" >catboost</th>\n",
       "      <td id=\"T_0f81c_row5_col0\" class=\"data row5 col0\" >CatBoost Classifier</td>\n",
       "      <td id=\"T_0f81c_row5_col1\" class=\"data row5 col1\" >0.8443</td>\n",
       "      <td id=\"T_0f81c_row5_col2\" class=\"data row5 col2\" >0.9180</td>\n",
       "      <td id=\"T_0f81c_row5_col3\" class=\"data row5 col3\" >0.7414</td>\n",
       "      <td id=\"T_0f81c_row5_col4\" class=\"data row5 col4\" >0.7798</td>\n",
       "      <td id=\"T_0f81c_row5_col5\" class=\"data row5 col5\" >0.7601</td>\n",
       "      <td id=\"T_0f81c_row5_col6\" class=\"data row5 col6\" >0.6450</td>\n",
       "      <td id=\"T_0f81c_row5_col7\" class=\"data row5 col7\" >0.6455</td>\n",
       "      <td id=\"T_0f81c_row5_col8\" class=\"data row5 col8\" >80.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row6\" class=\"row_heading level0 row6\" >xgboost</th>\n",
       "      <td id=\"T_0f81c_row6_col0\" class=\"data row6 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_0f81c_row6_col1\" class=\"data row6 col1\" >0.8394</td>\n",
       "      <td id=\"T_0f81c_row6_col2\" class=\"data row6 col2\" >0.9131</td>\n",
       "      <td id=\"T_0f81c_row6_col3\" class=\"data row6 col3\" >0.7299</td>\n",
       "      <td id=\"T_0f81c_row6_col4\" class=\"data row6 col4\" >0.7743</td>\n",
       "      <td id=\"T_0f81c_row6_col5\" class=\"data row6 col5\" >0.7515</td>\n",
       "      <td id=\"T_0f81c_row6_col6\" class=\"data row6 col6\" >0.6330</td>\n",
       "      <td id=\"T_0f81c_row6_col7\" class=\"data row6 col7\" >0.6336</td>\n",
       "      <td id=\"T_0f81c_row6_col8\" class=\"data row6 col8\" >6.8100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row7\" class=\"row_heading level0 row7\" >rf</th>\n",
       "      <td id=\"T_0f81c_row7_col0\" class=\"data row7 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_0f81c_row7_col1\" class=\"data row7 col1\" >0.8429</td>\n",
       "      <td id=\"T_0f81c_row7_col2\" class=\"data row7 col2\" >0.9046</td>\n",
       "      <td id=\"T_0f81c_row7_col3\" class=\"data row7 col3\" >0.7449</td>\n",
       "      <td id=\"T_0f81c_row7_col4\" class=\"data row7 col4\" >0.7743</td>\n",
       "      <td id=\"T_0f81c_row7_col5\" class=\"data row7 col5\" >0.7593</td>\n",
       "      <td id=\"T_0f81c_row7_col6\" class=\"data row7 col6\" >0.6428</td>\n",
       "      <td id=\"T_0f81c_row7_col7\" class=\"data row7 col7\" >0.6431</td>\n",
       "      <td id=\"T_0f81c_row7_col8\" class=\"data row7 col8\" >10.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row8\" class=\"row_heading level0 row8\" >et</th>\n",
       "      <td id=\"T_0f81c_row8_col0\" class=\"data row8 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_0f81c_row8_col1\" class=\"data row8 col1\" >0.8273</td>\n",
       "      <td id=\"T_0f81c_row8_col2\" class=\"data row8 col2\" >0.8955</td>\n",
       "      <td id=\"T_0f81c_row8_col3\" class=\"data row8 col3\" >0.6353</td>\n",
       "      <td id=\"T_0f81c_row8_col4\" class=\"data row8 col4\" >0.8046</td>\n",
       "      <td id=\"T_0f81c_row8_col5\" class=\"data row8 col5\" >0.7100</td>\n",
       "      <td id=\"T_0f81c_row8_col6\" class=\"data row8 col6\" >0.5895</td>\n",
       "      <td id=\"T_0f81c_row8_col7\" class=\"data row8 col7\" >0.5978</td>\n",
       "      <td id=\"T_0f81c_row8_col8\" class=\"data row8 col8\" >12.9950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n",
       "      <td id=\"T_0f81c_row9_col0\" class=\"data row9 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_0f81c_row9_col1\" class=\"data row9 col1\" >0.7283</td>\n",
       "      <td id=\"T_0f81c_row9_col2\" class=\"data row9 col2\" >0.8238</td>\n",
       "      <td id=\"T_0f81c_row9_col3\" class=\"data row9 col3\" >0.7833</td>\n",
       "      <td id=\"T_0f81c_row9_col4\" class=\"data row9 col4\" >0.5662</td>\n",
       "      <td id=\"T_0f81c_row9_col5\" class=\"data row9 col5\" >0.6573</td>\n",
       "      <td id=\"T_0f81c_row9_col6\" class=\"data row9 col6\" >0.4417</td>\n",
       "      <td id=\"T_0f81c_row9_col7\" class=\"data row9 col7\" >0.4577</td>\n",
       "      <td id=\"T_0f81c_row9_col8\" class=\"data row9 col8\" >7.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row10\" class=\"row_heading level0 row10\" >dt</th>\n",
       "      <td id=\"T_0f81c_row10_col0\" class=\"data row10 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_0f81c_row10_col1\" class=\"data row10 col1\" >0.7843</td>\n",
       "      <td id=\"T_0f81c_row10_col2\" class=\"data row10 col2\" >0.7590</td>\n",
       "      <td id=\"T_0f81c_row10_col3\" class=\"data row10 col3\" >0.6832</td>\n",
       "      <td id=\"T_0f81c_row10_col4\" class=\"data row10 col4\" >0.6733</td>\n",
       "      <td id=\"T_0f81c_row10_col5\" class=\"data row10 col5\" >0.6782</td>\n",
       "      <td id=\"T_0f81c_row10_col6\" class=\"data row10 col6\" >0.5160</td>\n",
       "      <td id=\"T_0f81c_row10_col7\" class=\"data row10 col7\" >0.5161</td>\n",
       "      <td id=\"T_0f81c_row10_col8\" class=\"data row10 col8\" >3.5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row11\" class=\"row_heading level0 row11\" >nb</th>\n",
       "      <td id=\"T_0f81c_row11_col0\" class=\"data row11 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_0f81c_row11_col1\" class=\"data row11 col1\" >0.4513</td>\n",
       "      <td id=\"T_0f81c_row11_col2\" class=\"data row11 col2\" >0.5764</td>\n",
       "      <td id=\"T_0f81c_row11_col3\" class=\"data row11 col3\" >0.7518</td>\n",
       "      <td id=\"T_0f81c_row11_col4\" class=\"data row11 col4\" >0.3492</td>\n",
       "      <td id=\"T_0f81c_row11_col5\" class=\"data row11 col5\" >0.4768</td>\n",
       "      <td id=\"T_0f81c_row11_col6\" class=\"data row11 col6\" >0.0414</td>\n",
       "      <td id=\"T_0f81c_row11_col7\" class=\"data row11 col7\" >0.0557</td>\n",
       "      <td id=\"T_0f81c_row11_col8\" class=\"data row11 col8\" >2.9950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row12\" class=\"row_heading level0 row12\" >knn</th>\n",
       "      <td id=\"T_0f81c_row12_col0\" class=\"data row12 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_0f81c_row12_col1\" class=\"data row12 col1\" >0.5232</td>\n",
       "      <td id=\"T_0f81c_row12_col2\" class=\"data row12 col2\" >0.5066</td>\n",
       "      <td id=\"T_0f81c_row12_col3\" class=\"data row12 col3\" >0.4528</td>\n",
       "      <td id=\"T_0f81c_row12_col4\" class=\"data row12 col4\" >0.3382</td>\n",
       "      <td id=\"T_0f81c_row12_col5\" class=\"data row12 col5\" >0.3872</td>\n",
       "      <td id=\"T_0f81c_row12_col6\" class=\"data row12 col6\" >0.0102</td>\n",
       "      <td id=\"T_0f81c_row12_col7\" class=\"data row12 col7\" >0.0105</td>\n",
       "      <td id=\"T_0f81c_row12_col8\" class=\"data row12 col8\" >8.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row13\" class=\"row_heading level0 row13\" >qda</th>\n",
       "      <td id=\"T_0f81c_row13_col0\" class=\"data row13 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_0f81c_row13_col1\" class=\"data row13 col1\" >0.5467</td>\n",
       "      <td id=\"T_0f81c_row13_col2\" class=\"data row13 col2\" >0.5044</td>\n",
       "      <td id=\"T_0f81c_row13_col3\" class=\"data row13 col3\" >0.3821</td>\n",
       "      <td id=\"T_0f81c_row13_col4\" class=\"data row13 col4\" >0.3338</td>\n",
       "      <td id=\"T_0f81c_row13_col5\" class=\"data row13 col5\" >0.2902</td>\n",
       "      <td id=\"T_0f81c_row13_col6\" class=\"data row13 col6\" >0.0088</td>\n",
       "      <td id=\"T_0f81c_row13_col7\" class=\"data row13 col7\" >0.0109</td>\n",
       "      <td id=\"T_0f81c_row13_col8\" class=\"data row13 col8\" >3.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row14\" class=\"row_heading level0 row14\" >dummy</th>\n",
       "      <td id=\"T_0f81c_row14_col0\" class=\"data row14 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_0f81c_row14_col1\" class=\"data row14 col1\" >0.6674</td>\n",
       "      <td id=\"T_0f81c_row14_col2\" class=\"data row14 col2\" >0.5000</td>\n",
       "      <td id=\"T_0f81c_row14_col3\" class=\"data row14 col3\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row14_col4\" class=\"data row14 col4\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row14_col5\" class=\"data row14 col5\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row14_col6\" class=\"data row14 col6\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row14_col7\" class=\"data row14 col7\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row14_col8\" class=\"data row14 col8\" >3.5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0f81c_level0_row15\" class=\"row_heading level0 row15\" >svm</th>\n",
       "      <td id=\"T_0f81c_row15_col0\" class=\"data row15 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_0f81c_row15_col1\" class=\"data row15 col1\" >0.3326</td>\n",
       "      <td id=\"T_0f81c_row15_col2\" class=\"data row15 col2\" >0.4941</td>\n",
       "      <td id=\"T_0f81c_row15_col3\" class=\"data row15 col3\" >1.0000</td>\n",
       "      <td id=\"T_0f81c_row15_col4\" class=\"data row15 col4\" >0.3326</td>\n",
       "      <td id=\"T_0f81c_row15_col5\" class=\"data row15 col5\" >0.4992</td>\n",
       "      <td id=\"T_0f81c_row15_col6\" class=\"data row15 col6\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row15_col7\" class=\"data row15 col7\" >0.0000</td>\n",
       "      <td id=\"T_0f81c_row15_col8\" class=\"data row15 col8\" >7.1150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21641eca510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_073c4_row2_col0, #T_073c4_row2_col1, #T_073c4_row2_col2, #T_073c4_row2_col3, #T_073c4_row2_col4, #T_073c4_row2_col5, #T_073c4_row2_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_073c4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_073c4_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_073c4_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_073c4_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_073c4_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_073c4_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_073c4_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_073c4_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_073c4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_073c4_row0_col0\" class=\"data row0 col0\" >0.8455</td>\n",
       "      <td id=\"T_073c4_row0_col1\" class=\"data row0 col1\" >0.9200</td>\n",
       "      <td id=\"T_073c4_row0_col2\" class=\"data row0 col2\" >0.7850</td>\n",
       "      <td id=\"T_073c4_row0_col3\" class=\"data row0 col3\" >0.7598</td>\n",
       "      <td id=\"T_073c4_row0_col4\" class=\"data row0 col4\" >0.7722</td>\n",
       "      <td id=\"T_073c4_row0_col5\" class=\"data row0 col5\" >0.6554</td>\n",
       "      <td id=\"T_073c4_row0_col6\" class=\"data row0 col6\" >0.6556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_073c4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_073c4_row1_col0\" class=\"data row1 col0\" >0.8432</td>\n",
       "      <td id=\"T_073c4_row1_col1\" class=\"data row1 col1\" >0.9211</td>\n",
       "      <td id=\"T_073c4_row1_col2\" class=\"data row1 col2\" >0.7615</td>\n",
       "      <td id=\"T_073c4_row1_col3\" class=\"data row1 col3\" >0.7649</td>\n",
       "      <td id=\"T_073c4_row1_col4\" class=\"data row1 col4\" >0.7632</td>\n",
       "      <td id=\"T_073c4_row1_col5\" class=\"data row1 col5\" >0.6460</td>\n",
       "      <td id=\"T_073c4_row1_col6\" class=\"data row1 col6\" >0.6460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_073c4_level0_row2\" class=\"row_heading level0 row2\" >Mean</th>\n",
       "      <td id=\"T_073c4_row2_col0\" class=\"data row2 col0\" >0.8444</td>\n",
       "      <td id=\"T_073c4_row2_col1\" class=\"data row2 col1\" >0.9205</td>\n",
       "      <td id=\"T_073c4_row2_col2\" class=\"data row2 col2\" >0.7732</td>\n",
       "      <td id=\"T_073c4_row2_col3\" class=\"data row2 col3\" >0.7624</td>\n",
       "      <td id=\"T_073c4_row2_col4\" class=\"data row2 col4\" >0.7677</td>\n",
       "      <td id=\"T_073c4_row2_col5\" class=\"data row2 col5\" >0.6507</td>\n",
       "      <td id=\"T_073c4_row2_col6\" class=\"data row2 col6\" >0.6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_073c4_level0_row3\" class=\"row_heading level0 row3\" >Std</th>\n",
       "      <td id=\"T_073c4_row3_col0\" class=\"data row3 col0\" >0.0012</td>\n",
       "      <td id=\"T_073c4_row3_col1\" class=\"data row3 col1\" >0.0006</td>\n",
       "      <td id=\"T_073c4_row3_col2\" class=\"data row3 col2\" >0.0118</td>\n",
       "      <td id=\"T_073c4_row3_col3\" class=\"data row3 col3\" >0.0025</td>\n",
       "      <td id=\"T_073c4_row3_col4\" class=\"data row3 col4\" >0.0045</td>\n",
       "      <td id=\"T_073c4_row3_col5\" class=\"data row3 col5\" >0.0047</td>\n",
       "      <td id=\"T_073c4_row3_col6\" class=\"data row3 col6\" >0.0048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21641f04290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>11:25:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Searching Hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Ridge Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         \n",
       "                                                                         \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                   11:25:35\n",
       "Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n",
       "Estimator  . . . . . . . . . . . . . . . . . .           Ridge Classifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc31b0648574fa8bd6159a234d20b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================\n",
    "# 3) Entrenamiento con PyCaret (robusto)\n",
    "# ========================================\n",
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model, blend_models,\n",
    "    finalize_model, save_model, pull, get_config\n",
    ")\n",
    "import inspect\n",
    "\n",
    "# Tipos expl√≠citos (categ√≥ricas como string, num√©ricas float)\n",
    "train_df = train_df.copy(deep=True)\n",
    "train_df[\"id_usuario\"] = train_df[\"id_usuario\"].astype(\"string\")\n",
    "for c in CAT_COLS_X:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = train_df[c].astype(\"string\").copy()\n",
    "for c in NUM_COLS_X:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = pd.to_numeric(train_df[c], errors=\"coerce\").astype(\"float64\").copy()\n",
    "\n",
    "# === 0) data_in: SOLO id_usuario + features + y_like (target al final) ===\n",
    "data_in = train_df[[\"id_usuario\"] + CAT_COLS_X + NUM_COLS_X + [\"y_like\"]].copy()\n",
    "\n",
    "# === 1) setup con kwargs ‚Äúseguros‚Äù seg√∫n la firma instalada ===\n",
    "desired_kwargs = dict(\n",
    "    data = data_in,\n",
    "    target = \"y_like\",\n",
    "    session_id = RANDOM_SEED,\n",
    "    fold = 2,\n",
    "    fold_strategy = \"groupkfold\",\n",
    "    fold_groups = \"id_usuario\",                 # agrupa por usuario en CV\n",
    "    categorical_features = CAT_COLS_X,\n",
    "    ignore_features = [\"id_usuario\"],           # nunca usar id como feature\n",
    "    remove_multicollinearity = True,\n",
    "    multicollinearity_threshold = 0.95,\n",
    "    numeric_imputation = \"mean\",\n",
    "    categorical_imputation = \"most_frequent\",\n",
    "    high_cardinality_features = [\"nombre_sitio\",\"x_tipoTur__tipoSit\",\"x_epoca__tipoSit\"],\n",
    "    high_cardinality_method = \"frequency\",\n",
    "    fix_imbalance = True,                       # si ves sobreajuste, prueba False\n",
    "    n_jobs = 1,                                 # para evitar issues de loky y dtypes\n",
    "    verbose = False,\n",
    "    use_gpu = False,\n",
    "    html = True,                                # tablas bonitas\n",
    ")\n",
    "sig = inspect.signature(setup)\n",
    "allowed = set(sig.parameters.keys())\n",
    "safe_kwargs = {k: v for k, v in desired_kwargs.items() if k in allowed}\n",
    "\n",
    "setup(**safe_kwargs)\n",
    "\n",
    "# === 2) comparar ‚Üí tunear ‚Üí blend ===\n",
    "best3 = compare_models(n_select=3, sort=\"AUC\")\n",
    "tuned = [tune_model(m, optimize=\"AUC\") for m in best3]\n",
    "blend = blend_models(tuned)\n",
    "\n",
    "# === 3) finalize y guardar pipeline ===\n",
    "final_cls = finalize_model(blend)\n",
    "MODEL_NAME = \"modelo_cls_like_v3\"\n",
    "save_model(final_cls, MODEL_NAME)\n",
    "print(f\"‚úÖ Modelo guardado: {MODEL_NAME}\")\n",
    "\n",
    "# === 4) Post-checks para evitar problemas en inferencia ===\n",
    "# 4.1) columnas de features que REALMENTE vio el prep de PyCaret\n",
    "X_cols = list(get_config(\"X_train\").columns)\n",
    "print(\"Features usadas por el pipeline (prep):\", len(X_cols))\n",
    "pd.Series(X_cols).to_csv(\"features_usadas_por_modelo.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 4.2) sanity: confirma que el target NO qued√≥ como feature en el prep\n",
    "bad_targets = [c for c in [\"y_like\",\"rating_usuario\",\"sitio_recomendado\"] if c in X_cols]\n",
    "if bad_targets:\n",
    "    print(\"‚ö†Ô∏è Aviso: el prep contiene columnas de target como features:\", bad_targets,\n",
    "          \"\\n   (seguir√° funcionando con el parche de inferencia en 2 etapas,\",\n",
    "          \"   pero considera re-entrenar para que no aparezcan ah√≠).\")\n",
    "\n",
    "# 4.3) guarda un ‚Äúmanifiesto‚Äù m√≠nimo para la simulaci√≥n\n",
    "manifest = {\n",
    "    \"cat_cols_x\": CAT_COLS_X,\n",
    "    \"num_cols_x\": NUM_COLS_X,\n",
    "    \"features_prep_seen\": X_cols,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"fold_strategy\": \"groupkfold\",\n",
    "    \"fold_groups\": \"id_usuario\",\n",
    "}\n",
    "pd.Series({k:str(v) for k,v in manifest.items()}).to_csv(\"manifest_modelo.csv\", encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_sin_sampler(model_pipeline, X_df):\n",
    "    \"\"\"\n",
    "    Aplica todos los pasos previos al estimador final, saltando cualquier paso\n",
    "    que requiera 'y' (samplers como FixImbalancer/SMOTE) o que tenga fit_resample.\n",
    "    Devuelve la matriz transformada (numpy o DataFrame seg√∫n el √∫ltimo paso).\n",
    "    \"\"\"\n",
    "    Xt = X_df\n",
    "    # recorre todas las etapas menos la √∫ltima (el estimador)\n",
    "    for name, trans in model_pipeline.steps[:-1]:\n",
    "        try:\n",
    "            # si el paso tiene fit_resample, es sampler -> saltar en inferencia\n",
    "            if hasattr(trans, \"fit_resample\"):\n",
    "                continue\n",
    "            # intenta transform(X)\n",
    "            Xt = trans.transform(Xt)\n",
    "        except TypeError as e:\n",
    "            # algunos transform piden y: intenta con y=None\n",
    "            try:\n",
    "                Xt = trans.transform(Xt, None)\n",
    "            except Exception:\n",
    "                # si aun as√≠ truena, lo saltamos\n",
    "                continue\n",
    "    return Xt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c410f4",
   "metadata": {},
   "source": [
    "## 4) Evaluaci√≥n del modelo guardado en TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cae6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4) Evaluaci√≥n en TEST (prep ‚Üí estimador)\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycaret.classification import load_model, get_config\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Helper: aplica el prep del pipeline saltando samplers (no van en inferencia) ---\n",
    "def _transform_sin_sampler(model_pipeline, X_df):\n",
    "    \"\"\"\n",
    "    Recorre todas las etapas del pipeline menos el estimador final y aplica transformaciones.\n",
    "    Si la etapa es un sampler (tiene fit_resample) se omite.\n",
    "    Si una etapa pide y en transform(), intenta con y=None; si no aplica, se omite.\n",
    "    Devuelve la matriz transformada (numpy/DF seg√∫n el √∫ltimo transformador).\n",
    "    \"\"\"\n",
    "    Xt = X_df\n",
    "    for name, trans in model_pipeline.steps[:-1]:\n",
    "        # 1) Omitir samplers / re-balanceadores\n",
    "        if hasattr(trans, \"fit_resample\"):\n",
    "            continue\n",
    "        # 2) Intentar transform\n",
    "        try:\n",
    "            Xt = trans.transform(Xt)\n",
    "        except TypeError:\n",
    "            # algunos wrappers piden y en transform; intentamos con y=None\n",
    "            try:\n",
    "                Xt = trans.transform(Xt, None)\n",
    "            except Exception:\n",
    "                # si tampoco aplica a inferencia, se omite\n",
    "                continue\n",
    "    return Xt\n",
    "\n",
    "# --- 1) Preprocesa test igual que train ---\n",
    "test_proc = test_df.copy(deep=True)\n",
    "test_proc = normalize_columns(test_proc)\n",
    "test_proc.columns = test_proc.columns.str.strip()\n",
    "test_proc = normalize_values(test_proc)\n",
    "test_proc = make_features(test_proc)\n",
    "\n",
    "# Tipos coherentes\n",
    "for c in CAT_COLS_X:\n",
    "    if c in test_proc.columns:\n",
    "        test_proc[c] = test_proc[c].astype(object)\n",
    "for c in NUM_COLS_X:\n",
    "    if c in test_proc.columns:\n",
    "        test_proc[c] = pd.to_numeric(test_proc[c], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# Target binario por si acaso\n",
    "if \"y_like\" not in test_proc.columns and \"rating_usuario\" in test_proc.columns:\n",
    "    test_proc[\"y_like\"] = (pd.to_numeric(test_proc[\"rating_usuario\"], errors=\"coerce\") >= 4).astype(int)\n",
    "\n",
    "# --- 2) Carga modelo y separa estimador final ---\n",
    "model = load_model(\"modelo_cls_like_v3\")\n",
    "assert hasattr(model, \"steps\") and len(model.steps) >= 1, \"Pipeline inv√°lido (sin steps).\"\n",
    "final_est = model.steps[-1][1]\n",
    "\n",
    "# --- 3) Alinear columnas a lo que vio el prep en train ---\n",
    "try:\n",
    "    feat_used = pd.read_csv(\"features_usadas_por_modelo.csv\", squeeze=True).tolist()\n",
    "except Exception:\n",
    "    feat_used = list(get_config(\"X_train\").columns)  # fallback si est√°s en la misma sesi√≥n de PyCaret\n",
    "\n",
    "Xinfer = test_proc.reindex(columns=feat_used).copy()\n",
    "\n",
    "# --- 4) Transformaci√≥n (saltando samplers) + scoring ---\n",
    "X_trans = _transform_sin_sampler(model, Xinfer)\n",
    "\n",
    "if hasattr(final_est, \"predict_proba\"):\n",
    "    proba = final_est.predict_proba(X_trans)\n",
    "    pos_idx = -1\n",
    "    if hasattr(final_est, \"classes_\"):\n",
    "        cls = np.array(final_est.classes_)\n",
    "        where = np.where(cls == 1)[0]\n",
    "        if len(where):\n",
    "            pos_idx = int(where[0])\n",
    "    score = proba[:, pos_idx].astype(float)\n",
    "\n",
    "elif hasattr(final_est, \"decision_function\"):\n",
    "    dfun = final_est.decision_function(X_trans)\n",
    "    dfun = np.asarray(dfun, dtype=float)\n",
    "    if dfun.ndim == 2 and dfun.shape[1] > 1:\n",
    "        pos_idx = -1\n",
    "        if hasattr(final_est, \"classes_\"):\n",
    "            cls = np.array(final_est.classes_)\n",
    "            where = np.where(cls == 1)[0]\n",
    "            if len(where):\n",
    "                pos_idx = int(where[0])\n",
    "        dfun = dfun[:, pos_idx]\n",
    "    score = 1.0/(1.0+np.exp(-dfun))\n",
    "\n",
    "else:\n",
    "    labels = final_est.predict(X_trans)\n",
    "    score = (pd.Series(labels).astype(str).isin([\"1\",\"True\",\"true\"])).astype(float).values\n",
    "\n",
    "# --- 5) M√©tricas Top-K + AUC ---\n",
    "df_scores = pd.DataFrame({\n",
    "    \"id_usuario\": test_proc[\"id_usuario\"].astype(str).values,\n",
    "    \"nombre_sitio\": test_proc[\"nombre_sitio\"].values,\n",
    "    \"score\": score,\n",
    "    \"relevancia\": test_proc[\"y_like\"].astype(float).values\n",
    "})\n",
    "\n",
    "results = {}\n",
    "for k in K_LIST:\n",
    "    rec_k = (df_scores.groupby(\"id_usuario\")\n",
    "             .apply(lambda g: recall_at_k(g, k, \"score\", \"relevancia\"))\n",
    "             .mean(skipna=True))\n",
    "    ndcg_k = (df_scores.groupby(\"id_usuario\")\n",
    "              .apply(lambda g: ndcg_at_k(g, k, \"score\", \"relevancia\"))\n",
    "              .mean(skipna=True))\n",
    "    cov_k = coverage_at_k(df_scores, k, \"score\", \"nombre_sitio\")\n",
    "    results[k] = {\"Recall@K\": rec_k, \"NDCG@K\": ndcg_k, \"Coverage@K\": cov_k}\n",
    "\n",
    "eval_df = pd.DataFrame(results).T\n",
    "print(\"\\n=== M√©tricas Top-K en TEST ===\")\n",
    "print(eval_df.round(4))\n",
    "\n",
    "try:\n",
    "    auc_global = roc_auc_score(df_scores[\"relevancia\"], df_scores[\"score\"])\n",
    "    print(\"\\nAUC global (binario):\", round(float(auc_global), 4))\n",
    "except Exception as e:\n",
    "    print(\"\\nAUC no disponible:\", e)\n",
    "\n",
    "print(\"\\nUsuarios test eval:\", df_scores[\"id_usuario\"].nunique())\n",
    "print(\"Items √∫nicos en test:\", df_scores[\"nombre_sitio\"].nunique())\n",
    "print(\"Tasa de positivos (y_like=1):\", round(float(df_scores[\"relevancia\"].mean()), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf23ae",
   "metadata": {},
   "source": [
    "## 5) An√°lisis adicionales (distribuci√≥n de positivos, m√©tricas por segmento, MAP/MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2623c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 5.1) Distribuci√≥n de positivos por usuario\n",
    "g = df_scores.groupby(\"id_usuario\")[\"relevancia\"].sum().rename(\"positivos_user\")\n",
    "print({\n",
    "    \"min\": int(g.min()), \"p25\": float(g.quantile(0.25)), \"mediana\": float(g.median()),\n",
    "    \"p75\": float(g.quantile(0.75)), \"max\": int(g.max())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# 5.2) M√©tricas por segmento (versi√≥n robusta a pandas)\n",
    "SEG = \"tipo_turista_preferido\"  # prueba tambi√©n con 'epoca_visita', 'ubicacion_geografica', etc.\n",
    "\n",
    "tmp = test_proc[[\"id_usuario\", SEG]].drop_duplicates()\n",
    "scores_seg = df_scores.merge(tmp, on=\"id_usuario\", how=\"left\")\n",
    "\n",
    "def eval_segment(seg_name):\n",
    "    out = {}\n",
    "    # agrupamos por (segmento, usuario)\n",
    "    by = scores_seg.groupby([seg_name, \"id_usuario\"], sort=False)\n",
    "\n",
    "    for k in [3, 5, 10]:\n",
    "        rec_user = by.apply(lambda g: recall_at_k(g, k, \"score\", \"relevancia\"))\n",
    "        ndc_user = by.apply(lambda g: ndcg_at_k(g, k, \"score\", \"relevancia\"))\n",
    "\n",
    "        # promedio por segmento (nivel 0 del √≠ndice jer√°rquico)\n",
    "        rec_mean = rec_user.groupby(level=0).agg(np.nanmean)\n",
    "        ndc_mean = ndc_user.groupby(level=0).agg(np.nanmean)\n",
    "\n",
    "        out[k] = pd.DataFrame({\"Recall@K\": rec_mean, \"NDCG@K\": ndc_mean}).sort_index()\n",
    "\n",
    "    return out\n",
    "\n",
    "seg_metrics = eval_segment(SEG)\n",
    "for k, dfk in seg_metrics.items():\n",
    "    print(f\"\\n== {SEG} @ {k} ==\")\n",
    "    print(dfk.round(3).sort_values(\"NDCG@K\", ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59febce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# 5.3) M√©tricas extra: HitRate, MAP@K, MRR@K\n",
    "def hitrate_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False).head(k)\n",
    "    return 1.0 if g[rel_col].sum() > 0 else 0.0\n",
    "\n",
    "def apk(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False)\n",
    "    rels = g[rel_col].values[:k]\n",
    "    if rels.sum() == 0: return 0.0\n",
    "    precs = [rels[:i+1].sum()/(i+1) for i in range(len(rels)) if rels[i]==1]\n",
    "    return float(np.mean(precs))\n",
    "\n",
    "def mrr_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False).head(k)\n",
    "    idx = np.where(g[rel_col].values==1)[0]\n",
    "    return 1.0/(idx[0]+1) if len(idx)>0 else 0.0\n",
    "\n",
    "more = {}\n",
    "for k in [3,5,10]:\n",
    "    hr = df_scores.groupby(\"id_usuario\").apply(lambda g: hitrate_at_k(g,k,\"score\",\"relevancia\")).mean()\n",
    "    mapk = df_scores.groupby(\"id_usuario\").apply(lambda g: apk(g,k,\"score\",\"relevancia\")).mean()\n",
    "    mrrk = df_scores.groupby(\"id_usuario\").apply(lambda g: mrr_at_k(g,k,\"score\",\"relevancia\")).mean()\n",
    "    more[k] = {\"HitRate@K\":hr, \"MAP@K\":mapk, \"MRR@K\":mrrk}\n",
    "pd.DataFrame(more).T.round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe000236",
   "metadata": {},
   "source": [
    "## 6) Simulaci√≥n: recomendaciones Top-N para un usuario nuevo (con diversidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Simulaci√≥n: recomendaciones Top-N para un usuario (MMR)\n",
    "# ============================================================\n",
    "TOP_N = 3\n",
    "DIVERSITY_LAMBDA = 0.25\n",
    "\n",
    "from pycaret.classification import load_model, get_config\n",
    "import numpy as np\n",
    "\n",
    "# --- si NO tienes el helper ya definido en el notebook, descomenta este bloque ---\n",
    "def _transform_sin_sampler(model_pipeline, X_df):\n",
    "    \"\"\"\n",
    "    Aplica todas las transformaciones previas al estimador final, saltando\n",
    "    cualquier paso que requiera 'y' (samplers) o tenga fit_resample.\n",
    "    \"\"\"\n",
    "    Xt = X_df\n",
    "    for name, trans in model_pipeline.steps[:-1]:\n",
    "        if hasattr(trans, \"fit_resample\"):   # sampler -> no va en inferencia\n",
    "            continue\n",
    "        try:\n",
    "            Xt = trans.transform(Xt)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                Xt = trans.transform(Xt, None)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return Xt\n",
    "\n",
    "def _ensure_catalog_schema(cat: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat = cat.copy()\n",
    "    needed = [\n",
    "        \"nombre_sitio\",\"tipo_sitio\",\"ubicacion_geografica\",\"clima_predominante\",\n",
    "        \"costo_entrada\",\"afluencia_promedio\",\"accesibilidad_general\",\n",
    "        \"duracion_esperada\",\"admite_mascotas\",\"idioma_info\"\n",
    "    ]\n",
    "    for col in needed:\n",
    "        if col not in cat.columns:\n",
    "            cat[col] = 0 if col in [\"costo_entrada\",\"afluencia_promedio\",\"duracion_esperada\",\"admite_mascotas\"] else \"\"\n",
    "    for c in [\"costo_entrada\",\"afluencia_promedio\",\"duracion_esperada\",\"admite_mascotas\"]:\n",
    "        cat[c] = pd.to_numeric(cat[c], errors=\"coerce\")\n",
    "    cat = normalize_values(cat)\n",
    "    cat = cat.loc[:, ~cat.columns.duplicated(keep='last')].copy()\n",
    "    return cat\n",
    "\n",
    "def _broadcast_user_over_catalog(user_profile: dict, catalog: pd.DataFrame) -> pd.DataFrame:\n",
    "    user_df = pd.DataFrame([user_profile])\n",
    "    user_df = normalize_values(user_df)\n",
    "    user_expanded = pd.concat([user_df]*len(catalog), ignore_index=True)\n",
    "    X = pd.concat([user_expanded.reset_index(drop=True), catalog.reset_index(drop=True)], axis=1)\n",
    "    X = X.loc[:, ~X.columns.duplicated(keep='last')].copy()\n",
    "    dup_cols = pd.Index(X.columns)[pd.Index(X.columns).duplicated(keep=False)]\n",
    "    if len(dup_cols) > 0:\n",
    "        raise RuntimeError(f\"A√∫n hay columnas duplicadas: {sorted(set(dup_cols))}\")\n",
    "    return X\n",
    "\n",
    "def _mmr_diversify(df_scored: pd.DataFrame, score_col: str, tipo_col: str = \"tipo_sitio\",\n",
    "                   top_n: int = 5, lam: float = 0.25) -> pd.DataFrame:\n",
    "    work = df_scored.copy()\n",
    "    chosen_idx, type_counts = [], {}\n",
    "    for _ in range(min(top_n, len(work))):\n",
    "        penal = work[tipo_col].map(lambda t: lam * type_counts.get(t, 0))\n",
    "        work[\"_adj\"] = work[score_col] - penal\n",
    "        pick = work[\"_adj\"].idxmax()\n",
    "        chosen_idx.append(pick)\n",
    "        t = work.at[pick, tipo_col]\n",
    "        type_counts[t] = type_counts.get(t, 0) + 1\n",
    "        work = work.drop(index=pick)\n",
    "    return df_scored.loc[chosen_idx]\n",
    "\n",
    "def recommend_for_user(user_profile: dict, model_name: str = \"modelo_cls_like_v3\",\n",
    "                       catalog_path: str = CAT_PATH, top_n: int = TOP_N,\n",
    "                       diversity_lambda: float = DIVERSITY_LAMBDA) -> pd.DataFrame:\n",
    "    # 1) cat√°logo + normalizaci√≥n\n",
    "    cat = pd.read_csv(catalog_path, sep=SEP, encoding=ENC)\n",
    "    cat = normalize_columns(cat); cat.columns = cat.columns.str.strip()\n",
    "    cat = _ensure_catalog_schema(cat)\n",
    "\n",
    "    # 2) producto usuario √ó cat√°logo + features\n",
    "    X = _broadcast_user_over_catalog(user_profile, cat)\n",
    "    X = make_features(X)\n",
    "\n",
    "    # 3) tipos coherentes\n",
    "    for c in CAT_COLS_X:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(object)\n",
    "    for c in NUM_COLS_X:\n",
    "        if c in X.columns:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "    # 4) cargar modelo y estimador final\n",
    "    model = load_model(model_name)\n",
    "    final_est = model.steps[-1][1]\n",
    "\n",
    "    # 5) alinear columnas a las que vio el prep en train\n",
    "    try:\n",
    "        feat_used = pd.read_csv(\"features_usadas_por_modelo.csv\", header=None).squeeze(\"columns\").tolist()\n",
    "    except Exception:\n",
    "        feat_used = list(get_config(\"X_train\").columns)\n",
    "    Xinfer = X.reindex(columns=feat_used, fill_value=np.nan).copy()\n",
    "\n",
    "    # 6) transformar saltando samplers + score\n",
    "    X_trans = _transform_sin_sampler(model, Xinfer)\n",
    "\n",
    "    if hasattr(final_est, \"predict_proba\"):\n",
    "        proba = final_est.predict_proba(X_trans)\n",
    "        pos_idx = -1\n",
    "        if hasattr(final_est, \"classes_\"):\n",
    "            cls = np.array(final_est.classes_); where = np.where(cls==1)[0]\n",
    "            if len(where): pos_idx = int(where[0])\n",
    "        score = proba[:, pos_idx].astype(float)\n",
    "    elif hasattr(final_est, \"decision_function\"):\n",
    "        dfun = final_est.decision_function(X_trans)\n",
    "        dfun = np.asarray(dfun, dtype=float)\n",
    "        if dfun.ndim == 2 and dfun.shape[1] > 1:\n",
    "            pos_idx = -1\n",
    "            if hasattr(final_est, \"classes_\"):\n",
    "                cls = np.array(final_est.classes_); where = np.where(cls==1)[0]\n",
    "                if len(where): pos_idx = int(where[0])\n",
    "            dfun = dfun[:, pos_idx]\n",
    "        score = 1.0/(1.0+np.exp(-dfun))\n",
    "    else:\n",
    "        labels = final_est.predict(X_trans)\n",
    "        score = (pd.Series(labels).astype(str).isin([\"1\",\"True\",\"true\"])).astype(float).values\n",
    "\n",
    "    # 7) salida + re-rank diversificado\n",
    "    out = pd.DataFrame({\n",
    "        \"nombre_sitio\": X[\"nombre_sitio\"].values,\n",
    "        \"tipo_sitio\":   X[\"tipo_sitio\"].values,\n",
    "        \"ubicacion_geografica\": X.get(\"ubicacion_geografica\", pd.Series([\"\"]*len(X))).values,\n",
    "        \"costo_entrada\": pd.to_numeric(X.get(\"costo_entrada\", pd.Series([0]*len(X))), errors=\"coerce\"),\n",
    "        \"admite_mascotas\": pd.to_numeric(X.get(\"admite_mascotas\", pd.Series([0]*len(X))), errors=\"coerce\"),\n",
    "        \"idioma_info\": X.get(\"idioma_info\", pd.Series([\"\"]*len(X))).values,\n",
    "        \"score_like\": score\n",
    "    })\n",
    "    out = out.sort_values(\"score_like\", ascending=False).reset_index(drop=True)\n",
    "    out_div = _mmr_diversify(out, score_col=\"score_like\", tipo_col=\"tipo_sitio\",\n",
    "                             top_n=top_n, lam=diversity_lambda).reset_index(drop=True)\n",
    "    out_div[\"score_like\"] = out_div[\"score_like\"].round(4)\n",
    "    return out_div\n",
    "\n",
    "# Perfiles de prueba\n",
    "usuario_cultural = {\n",
    "    \"id_usuario\": \"SIMU-002\", \"nacionalidad\": \"Colombia\", \"origen\": \"Tunja\",\n",
    "    \"tipo_turista_preferido\": \"cultura\", \"compania_viaje\": \"familia\",\n",
    "    \"restricciones_movilidad\": \"ninguna\", \"epoca_visita\": \"puente_festivo\",\n",
    "    \"presupuesto_estimado\": 120000, \"frecuencia_viaje\": 1, \"sitios_visitados\": 2,\n",
    "    \"calificacion_sitios_previos\": 3.8, \"tiempo_estancia_promedio\": 1, \"edad\": 36\n",
    "}\n",
    "usuario_naturaleza = {\n",
    "    \"id_usuario\": \"SIMU-002\", \"nacionalidad\": \"Colombia\", \"origen\": \"Tunja\",\n",
    "    \"tipo_turista_preferido\": \"aventura\", \"compania_viaje\": \"pareja\",\n",
    "    \"restricciones_movilidad\": \"ninguna\", \"epoca_visita\": \"puente_festivo\",\n",
    "    \"presupuesto_estimado\": 120000, \"edad\": 36\n",
    "}\n",
    "\n",
    "print(\"\\n=== Recomendaciones PERFIL 1 ===\")\n",
    "display(recommend_for_user(usuario_cultural))\n",
    "print(\"\\n=== Recomendaciones PERFIL 2 ===\")\n",
    "display(recommend_for_user(usuario_naturaleza))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "villaIA_leyva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
