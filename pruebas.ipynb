{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports & config ---\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "DATA_PATH = \"dataset_Recomendacion_villa_de_leyva_eleccion (2).csv\"   # ajústalo\n",
    "CAT_PATH  = \"catalogo_vdl_lugares_unico.csv\"               # ajústalo\n",
    "SEP, ENC  = \";\", \"utf-8-sig\"\n",
    "RANDOM_SEED = 42\n",
    "K_LIST = [3, 5, 10]\n",
    "TEST_USER_FRAC = 0.20     # % de usuarios para hold-out honesto\n",
    "\n",
    "# --- normalizador de encabezados con caracteres raros ---\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ren = {\n",
    "        \"compa¤¡a_viaje\": \"compania_viaje\",\n",
    "        \"‚poca_visita\": \"epoca_visita\",\n",
    "    }\n",
    "    ren = {k:v for k,v in ren.items() if k in df.columns}\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "# --- métricas Top-K ---\n",
    "def _dcg_at_k(rels): return float(np.sum([r/np.log2(i+2) for i,r in enumerate(rels)]))\n",
    "\n",
    "def recall_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False)\n",
    "    topk = g.head(k)\n",
    "    tot = g[rel_col].sum()\n",
    "    return float(\"nan\") if tot==0 else float(topk[rel_col].sum()/tot)\n",
    "\n",
    "def ndcg_at_k(g, k, score_col, rel_col):\n",
    "    g = g.sort_values(score_col, ascending=False)\n",
    "    dcg  = _dcg_at_k(g.head(k)[rel_col].tolist())\n",
    "    idcg = _dcg_at_k(sorted(g[rel_col].tolist(), reverse=True)[:k])\n",
    "    return float(\"nan\") if idcg==0 else float(dcg/idcg)\n",
    "\n",
    "def coverage_at_k(df, k, score_col, item_col=\"nombre_sitio\"):\n",
    "    topk = (df.sort_values([\"id_usuario\", score_col], ascending=[True, False])\n",
    "              .groupby(\"id_usuario\").head(k))\n",
    "    return float(topk[item_col].nunique() / df[item_col].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config de lectura ---\n",
    "ENC = \"utf-8-sig\"\n",
    "SEP = \";\"\n",
    "DATA_PATH = \"dataset_Recomendacion_villa_de_leyva_eleccion (2).csv\"  # <-- usa el tuyo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# (opcional) inspección rápida\n",
    "# with open(DATA_PATH, \"r\", encoding=ENC) as f:\n",
    "#     for _ in range(3): print(f.readline().rstrip(\"\\n\"))\n",
    "\n",
    "# --- carga ---\n",
    "df = pd.read_csv(DATA_PATH, sep=SEP, encoding=ENC)\n",
    "\n",
    "# Normaliza nombres (tu función)\n",
    "df = normalize_columns(df)        # asegura minúsculas, sin acentos/espacios\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# ---- Parche de esquema: renombrar sinónimos / grafías esperadas ----\n",
    "CANDIDATES = {\n",
    "    \"compania_viaje\":    [\"compania_viaje\", \"compañia_viaje\", \"compan_a_viaje\", \"companaviaje\"],\n",
    "    \"costo_entrada\":     [\"costo_entrada\", \"costoentrada\", \"precio_entrada\", \"costo\"],\n",
    "    \"afluencia_promedio\":[\"afluencia_promedio\", \"afluencia\", \"afluencia_prom\"],\n",
    "    \"duracion_esperada\": [\"duracion_esperada\", \"duracion_estimada\", \"duracion\", \"tiempo_esperado\"],\n",
    "    \"presupuesto_estimado\": [\"presupuesto_estimado\",\"presupuesto\",\"budget\"],\n",
    "    \"tipo_turista_preferido\": [\"tipo_turista_preferido\",\"preferencia\",\"perfil_preferido\"],\n",
    "}\n",
    "\n",
    "for tgt, opts in CANDIDATES.items():\n",
    "    if tgt not in df.columns:\n",
    "        for c in opts:\n",
    "            if c in df.columns:\n",
    "                df = df.rename(columns={c: tgt})\n",
    "                break\n",
    "\n",
    "# Validación mínima antes de features:\n",
    "REQ = [\"costo_entrada\",\"presupuesto_estimado\",\"tipo_sitio\",\n",
    "       \"tipo_turista_preferido\",\"epoca_visita\"]\n",
    "missing = [c for c in REQ if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Faltan columnas obligatorias: {missing}\\nDisponibles: {list(df.columns)}\")\n",
    "\n",
    "# Asegura tipos numéricos (por si llegaron como texto)\n",
    "for c in [\"costo_entrada\",\"presupuesto_estimado\",\"edad\",\"frecuencia_viaje\",\n",
    "          \"sitios_visitados\",\"calificacion_sitios_previos\",\n",
    "          \"tiempo_estancia_promedio\",\"afluencia_promedio\",\n",
    "          \"duracion_esperada\",\"admite_mascotas\",\"rating_usuario\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# quita posibles fugas si llegara a existir\n",
    "if \"sitio_recomendado\" in df.columns and df[\"sitio_recomendado\"].dtype == object:\n",
    "    # si por error viene con nombres de sitio, repara usando rating>=4\n",
    "    df[\"sitio_recomendado\"] = (df[\"rating_usuario\"] >= 4.0).astype(int)\n",
    "\n",
    "# --- columnas por tipo (ya con nombres normalizados) ---\n",
    "CAT_COLS = [\n",
    "    \"nacionalidad\",\"origen\",\"tipo_turista_preferido\",\"compania_viaje\",\n",
    "    \"restricciones_movilidad\",\"nombre_sitio\",\"tipo_sitio\",\"accesibilidad_general\",\n",
    "    \"idioma_info\",\"ubicacion_geografica\",\"clima_predominante\",\"epoca_visita\"\n",
    "]\n",
    "NUM_COLS = [\n",
    "    \"edad\",\"frecuencia_viaje\",\"presupuesto_estimado\",\"sitios_visitados\",\n",
    "    \"calificacion_sitios_previos\",\"tiempo_estancia_promedio\",\"costo_entrada\",\n",
    "    \"afluencia_promedio\",\"duracion_esperada\",\"admite_mascotas\"\n",
    "]\n",
    "\n",
    "# --- AFINIDAD (actualizado a tus tipos reales) ---\n",
    "AFINIDAD = {\n",
    "    \"cultural\": {\"museo\":0.9,\"centro_historico\":0.9,\"arquitectura\":0.85,\"arqueologico\":0.85,\"plaza\":0.7,\"religioso\":0.7},\n",
    "    \"naturaleza\": {\"naturaleza\":0.95,\"senderismo\":0.9,\"mirador\":0.8},\n",
    "    \"aventura\": {\"senderismo\":0.9,\"parque_tematico\":0.75,\"mirador\":0.75,\"naturaleza\":0.7},\n",
    "    \"gastronomico\": {\"gastronomico\":0.95},\n",
    "    \"relax\": {\"mirador\":0.9,\"plaza\":0.8,\"naturaleza\":0.75,\"arquitectura\":0.8},\n",
    "}\n",
    "\n",
    "def make_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # Costo relativo (evita división por cero)\n",
    "    denom = (X[\"presupuesto_estimado\"]*0.15).replace(0, np.nan)\n",
    "    X[\"ratio_costo_presu\"] = (X[\"costo_entrada\"] / denom).clip(0, 3).fillna(0)\n",
    "    # Afinidad perfil–tipo\n",
    "    X[\"afinidad_tipo\"] = X.apply(\n",
    "        lambda r: AFINIDAD.get(str(r[\"tipo_turista_preferido\"]), {}).get(str(r[\"tipo_sitio\"]), 0.5), axis=1\n",
    "    )\n",
    "    # Cruces categóricos\n",
    "    X[\"x_tipoTur__tipoSit\"] = X[\"tipo_turista_preferido\"].astype(str) + \"×\" + X[\"tipo_sitio\"].astype(str)\n",
    "    X[\"x_epoca__tipoSit\"]   = X[\"epoca_visita\"].astype(str) + \"×\" + X[\"tipo_sitio\"].astype(str)\n",
    "    return X\n",
    "\n",
    "df = make_features(df)\n",
    "\n",
    "# Target binario para la tarea de clasificación\n",
    "df[\"y_like\"] = (df[\"rating_usuario\"] >= 4.0).astype(int)\n",
    "\n",
    "# Columnas extendidas para el modelado\n",
    "CAT_COLS_X = CAT_COLS + [\"x_tipoTur__tipoSit\",\"x_epoca__tipoSit\"]\n",
    "NUM_COLS_X = NUM_COLS + [\"ratio_costo_presu\",\"afinidad_tipo\"]\n",
    "\n",
    "print(\"✅ Esquema OK. Ejemplo columnas:\", df.columns[:15].tolist())\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f943d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- carga ---\n",
    "df = pd.read_csv(DATA_PATH, sep=SEP, encoding=ENC)\n",
    "df = normalize_columns(df)\n",
    "\n",
    "# quita posibles fugas\n",
    "if \"sitio_recomendado\" in df.columns:\n",
    "    df = df.drop(columns=[\"sitio_recomendado\"])\n",
    "\n",
    "# columnas por tipo (ajústalas si cambian en tu dataset)\n",
    "CAT_COLS = [\n",
    "    \"nacionalidad\",\"origen\",\"tipo_turista_preferido\",\"compañia_viaje\",\n",
    "    \"restricciones_movilidad\",\"nombre_sitio\",\"tipo_sitio\",\"accesibilidad_general\",\n",
    "    \"idioma_info\",\"ubicacion_geografica\",\"clima_predominante\",\"epoca_visita\"\n",
    "]\n",
    "NUM_COLS = [\n",
    "    \"edad\",\"frecuencia_viaje\",\"presupuesto_estimado\",\"sitios_visitados\",\n",
    "    \"calificacion_sitios_previos\",\"tiempo_estancia_promedio\",\"costo_entrada\",\n",
    "    \"afluencia_promedio\",\"duracion_esperada\",\"admite_mascotas\"\n",
    "]\n",
    "\n",
    "# --- features de interacción usuario×sitio ---\n",
    "AFINIDAD = {\n",
    "    \"cultural\": {\"museo\":0.9,\"histórico\":0.9,\"religioso\":0.7,\"arquitectura\":0.85,\"museo_religioso\":0.8,\"arqueologico\":0.85,\"plaza\":0.7},\n",
    "    \"naturaleza\": {\"natural\":0.95,\"senderismo\":0.9,\"mirador\":0.8,\"parque_urbano\":0.6},\n",
    "    \"aventura\": {\"aventura\":0.95,\"senderismo\":0.85,\"parque_tematico\":0.7},\n",
    "    \"gastronómico\": {\"gastronomico\":0.95,\"enoturismo\":0.9,\"artesanal\":0.6,\"plaza\":0.6},\n",
    "    \"relax_fotografía\": {\"mirador\":0.9,\"plaza\":0.8,\"arquitectura\":0.8,\"natural\":0.75},\n",
    "}\n",
    "\n",
    "def make_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # costo relativo al 15% del presupuesto diario\n",
    "    X[\"ratio_costo_presu\"] = (X[\"costo_entrada\"] / (X[\"presupuesto_estimado\"]*0.15)).clip(0, 3)\n",
    "    # afinidad perfil-tipo_sitio\n",
    "    X[\"afinidad_tipo\"] = X.apply(lambda r: AFINIDAD.get(r[\"tipo_turista_preferido\"],{}).get(r[\"tipo_sitio\"],0.5), axis=1)\n",
    "    # cruces categóricos (ayuda a modelos lineales y deja explícita la interacción)\n",
    "    X[\"x_tipoTur__tipoSit\"] = X[\"tipo_turista_preferido\"] + \"×\" + X[\"tipo_sitio\"]\n",
    "    X[\"x_epoca__tipoSit\"]    = X[\"epoca_visita\"] + \"×\" + X[\"tipo_sitio\"]\n",
    "    return X\n",
    "\n",
    "df = make_features(df)\n",
    "\n",
    "# añade variable binaria \"like\" para clasificación\n",
    "df[\"y_like\"] = (df[\"rating_usuario\"] >= 4.0).astype(int)\n",
    "\n",
    "# columnas extendidas\n",
    "CAT_COLS_X = CAT_COLS + [\"x_tipoTur__tipoSit\",\"x_epoca__tipoSit\"]\n",
    "NUM_COLS_X = NUM_COLS + [\"ratio_costo_presu\",\"afinidad_tipo\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c92cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "users = df[\"id_usuario\"].drop_duplicates().to_numpy()\n",
    "test_users = set(rng.choice(users, size=int(len(users)*TEST_USER_FRAC), replace=False))\n",
    "\n",
    "train_df = df[~df[\"id_usuario\"].isin(test_users)].reset_index(drop=True)\n",
    "test_df  = df[ df[\"id_usuario\"].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(\"Usuarios train/test:\", train_df[\"id_usuario\"].nunique(), test_df[\"id_usuario\"].nunique())\n",
    "print(\"Filas train/test:\", train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e547f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model, blend_models,\n",
    "    finalize_model, predict_model, pull, save_model\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Copias profundas y tipos explícitos (evita vistas read-only)\n",
    "train_df = train_df.copy(deep=True)\n",
    "\n",
    "# Asegura que id_usuario esté como string (pero NO como feature)\n",
    "train_df[\"id_usuario\"] = train_df[\"id_usuario\"].astype(\"string\")\n",
    "\n",
    "for c in CAT_COLS_X:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = train_df[c].astype(\"string\").copy()\n",
    "\n",
    "for c in NUM_COLS_X:\n",
    "    if c in train_df.columns:\n",
    "        train_df[c] = pd.to_numeric(train_df[c], errors=\"coerce\").astype(\"float64\").copy()\n",
    "\n",
    "# 2) Setup — sin SMOTE y sin paralelismo en la primera corrida\n",
    "setup_cls = setup(\n",
    "    data = train_df[[\"id_usuario\"] + CAT_COLS_X + NUM_COLS_X + [\"y_like\"]].copy(),\n",
    "    target = \"y_like\",\n",
    "    fold = 5,\n",
    "    fold_strategy = \"groupkfold\",\n",
    "    fold_groups = \"id_usuario\",          # usa el id para agrupar\n",
    "    categorical_features = CAT_COLS_X,\n",
    "    ignore_features = [\"id_usuario\"],     # << clave: no lo pases como feature\n",
    "    remove_multicollinearity = True,\n",
    "    multicollinearity_threshold = 0.95,\n",
    "    imputation_type = \"simple\",\n",
    "    fix_imbalance = False,                # << desactivar por ahora\n",
    "    n_jobs = 1,                           # << sin paralelismo (evita bug loky/writeable)\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "best3 = compare_models(n_select=3, sort=\"AUC\")\n",
    "tuned = [tune_model(m, optimize=\"AUC\") for m in best3]\n",
    "blend = blend_models(tuned)\n",
    "final_cls = finalize_model(blend)\n",
    "save_model(final_cls, \"modelo_cls_like_v2\")\n",
    "\n",
    "# Si esto corre OK, ya puedes volver a activar fix_imbalance=True y/o subir n_jobs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136535b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= PARCHE REGRESIÓN =================\n",
    "from pycaret.regression import (\n",
    "    setup as setup_reg, compare_models as compare_reg, tune_model as tune_reg,\n",
    "    blend_models as blend_reg, finalize_model as finalize_reg, predict_model as predict_reg,\n",
    "    pull as pull_reg, save_model as save_reg\n",
    ")\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# 0) Preparar train con copias profundas y tipos explícitos\n",
    "train_reg = train_df[[\"id_usuario\"] + CAT_COLS_X + NUM_COLS_X + [\"rating_usuario\"]].copy(deep=True)\n",
    "\n",
    "# Asegurar tipos\n",
    "train_reg[\"id_usuario\"] = train_reg[\"id_usuario\"].astype(\"string\")\n",
    "for c in CAT_COLS_X:\n",
    "    if c in train_reg.columns:\n",
    "        train_reg[c] = train_reg[c].astype(\"string\").copy()\n",
    "for c in NUM_COLS_X:\n",
    "    if c in train_reg.columns:\n",
    "        train_reg[c] = pd.to_numeric(train_reg[c], errors=\"coerce\").astype(\"float64\").copy()\n",
    "\n",
    "# Target numérico y sin NaNs / inf\n",
    "train_reg[\"rating_usuario\"] = pd.to_numeric(train_reg[\"rating_usuario\"], errors=\"coerce\").astype(\"float64\")\n",
    "train_reg = train_reg.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 1) Setup — sin paralelismo, ignorando id_usuario (y opcionalmente nombre_sitio)\n",
    "IGNORE_FEATS = [\"id_usuario\"]  # agrega \"nombre_sitio\" si quieres evitar OHE de alta cardinalidad\n",
    "setup_reg(\n",
    "    data = train_reg,\n",
    "    target = \"rating_usuario\",\n",
    "    session_id = RANDOM_SEED,\n",
    "    fold = 5,\n",
    "    fold_strategy = \"groupkfold\",\n",
    "    fold_groups = \"id_usuario\",\n",
    "    categorical_features = CAT_COLS_X,\n",
    "    ignore_features = IGNORE_FEATS,\n",
    "    remove_multicollinearity = True,\n",
    "    multicollinearity_threshold = 0.95,\n",
    "    imputation_type = \"simple\",\n",
    "    n_jobs = 1,            # evita errores de loky/writeable en 1ra pasada\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "# 2) Benchmark, tuning y ensamble\n",
    "best3r = compare_reg(n_select=3, sort=\"RMSE\")\n",
    "lb_reg = pull_reg(); lb_reg.to_csv(\"leaderboard_regresion.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "tunedr = [tune_reg(m, optimize=\"RMSE\") for m in best3r]\n",
    "blendr = blend_reg(tunedr)\n",
    "final_reg = finalize_reg(blendr)\n",
    "save_reg(final_reg, \"modelo_reg_rating_v2\")\n",
    "\n",
    "# 3) Evaluación en TEST (alineamiento seguro para métricas Top-K)\n",
    "# --- evaluación en TEST (sin columnas duplicadas) ---\n",
    "Xtest = test_df[[\"id_usuario\"] + CAT_COLS_X + NUM_COLS_X].copy(deep=True)\n",
    "Xtest[\"id_usuario\"] = Xtest[\"id_usuario\"].astype(\"string\")\n",
    "for c in CAT_COLS_X:\n",
    "    if c in Xtest.columns:\n",
    "        Xtest[c] = Xtest[c].astype(\"string\")\n",
    "for c in NUM_COLS_X:\n",
    "    if c in Xtest.columns:\n",
    "        Xtest[c] = pd.to_numeric(Xtest[c], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# Guardamos ytest SOLO con la verdad (rating_usuario) para evitar duplicados\n",
    "ytest = test_df[[\"rating_usuario\"]].reset_index(drop=True)\n",
    "\n",
    "# Predicción (predict_model devuelve X original + columna 'prediction_label')\n",
    "pred = predict_reg(final_reg, data=Xtest).reset_index(drop=True)\n",
    "pred = pred.rename(columns={\"prediction_label\": \"rating_prev\"})\n",
    "\n",
    "# Nos quedamos con UNA SOLA columna de id y de item:\n",
    "# - Si 'nombre_sitio' está en pred (porque lo usamos como feature), lo conservamos de ahí.\n",
    "# - Si NO está (porque lo ignoraste), lo traemos desde test_df.\n",
    "cols_keep = [\"id_usuario\", \"rating_prev\"]\n",
    "if \"nombre_sitio\" in pred.columns:\n",
    "    cols_keep.append(\"nombre_sitio\")\n",
    "test_pred = pred[cols_keep].copy()\n",
    "\n",
    "if \"nombre_sitio\" not in test_pred.columns:\n",
    "    test_pred = pd.concat([test_pred, test_df[[\"nombre_sitio\"]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Añadimos la verdad del rating sin duplicar id/ítem\n",
    "test_pred[\"rating_usuario\"] = ytest[\"rating_usuario\"].values\n",
    "test_pred[\"y_true_rel\"] = (test_pred[\"rating_usuario\"] >= 4.0).astype(int)\n",
    "\n",
    "# (opcional) sanity check: que sólo haya UNA columna 'id_usuario' y 'nombre_sitio'\n",
    "# print([c for c in test_pred.columns if c == \"id_usuario\"])\n",
    "# print([c for c in test_pred.columns if c == \"nombre_sitio\"])\n",
    "\n",
    "# --- Métricas Top-K ---\n",
    "metrics_reg = {}\n",
    "for K in K_LIST:\n",
    "    recalls = [recall_at_k(g, K, \"rating_prev\", \"y_true_rel\") for _, g in test_pred.groupby(\"id_usuario\")]\n",
    "    ndcgs   = [ndcg_at_k(g,   K, \"rating_prev\", \"y_true_rel\") for _, g in test_pred.groupby(\"id_usuario\")]\n",
    "    cov     = coverage_at_k(test_pred, K, \"rating_prev\", \"nombre_sitio\")\n",
    "    metrics_reg[K] = {\"recall\": float(np.nanmean(recalls)),\n",
    "                      \"ndcg\":   float(np.nanmean(ndcgs)),\n",
    "                      \"coverage\": cov}\n",
    "\n",
    "import json\n",
    "print(\"Top-K REGRESIÓN (global):\", json.dumps(metrics_reg, indent=2, ensure_ascii=False))\n",
    "with open(\"metrics_regresion_topk.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_reg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ================= FIN PARCHE =================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycaret.classification import load_model as load_cls, predict_model as predict_cls\n",
    "from pycaret.regression import     load_model as load_reg,  predict_model as predict_reg\n",
    "\n",
    "CLS = load_cls(\"modelo_cls_like_v2\")\n",
    "REG = load_reg(\"modelo_reg_rating_v2\")\n",
    "CAT = pd.read_csv(CAT_PATH, sep=SEP, encoding=ENC)\n",
    "\n",
    "IGNORED_AT_TRAIN = [\"id_usuario\"]  # se ignoró en setup -> NO pasarla al pipeline\n",
    "\n",
    "def _build_candidates(user: dict) -> pd.DataFrame:\n",
    "    cand = CAT.copy()\n",
    "    for k, v in user.items():\n",
    "        cand[k] = v\n",
    "    cand = normalize_columns(cand)\n",
    "    cand = make_features(cand)                 # mismas features que en train\n",
    "    return cand\n",
    "\n",
    "def _prepare_for_pipeline(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # quitar columnas ignoradas en setup\n",
    "    X.drop(columns=[c for c in IGNORED_AT_TRAIN if c in X.columns], inplace=True, errors=\"ignore\")\n",
    "    # (opcional) asegurar tipos como en train\n",
    "    for c in CAT_COLS_X:\n",
    "        if c in X.columns: X[c] = X[c].astype(\"string\")\n",
    "    for c in NUM_COLS_X:\n",
    "        if c in X.columns: X[c] = pd.to_numeric(X[c], errors=\"coerce\").astype(\"float64\")\n",
    "    return X\n",
    "\n",
    "def _prob_like_from_hard_voting(pipeline, Xdf: pd.DataFrame) -> pd.Series:\n",
    "    # 1) preprocesa con el pipeline sin el último paso (modelo)\n",
    "    pre = pipeline[:-1]\n",
    "    Xenc = pre.transform(Xdf)\n",
    "\n",
    "    clf = pipeline.named_steps.get('trained_model', pipeline.steps[-1][1])\n",
    "    proba_list = []\n",
    "\n",
    "    for est in getattr(clf, 'estimators_', []):\n",
    "        if hasattr(est, \"predict_proba\"):\n",
    "            p = est.predict_proba(Xenc)\n",
    "            classes = getattr(est, \"classes_\", None)\n",
    "            if classes is not None:\n",
    "                cls_list = list(classes)\n",
    "                if 1 in cls_list: pos = cls_list.index(1)\n",
    "                elif \"1\" in cls_list: pos = cls_list.index(\"1\")\n",
    "                else: pos = p.shape[1]-1\n",
    "            else:\n",
    "                pos = p.shape[1]-1\n",
    "            proba_list.append(p[:, pos])\n",
    "        elif hasattr(est, \"decision_function\"):\n",
    "            df = est.decision_function(Xenc)\n",
    "            if df.ndim == 1:\n",
    "                proba_list.append(1.0/(1.0 + np.exp(-df)))            # logística\n",
    "            else:\n",
    "                col = 1 if df.shape[1] > 1 else 0\n",
    "                proba_list.append(1.0/(1.0 + np.exp(-df[:, col])))\n",
    "\n",
    "    if proba_list:\n",
    "        return pd.Series(np.mean(np.column_stack(proba_list), axis=1), index=Xdf.index)\n",
    "\n",
    "    # último recurso: usar predict_model por si expone alguna columna de score\n",
    "    scored = predict_cls(CLS, data=Xdf, raw_score=True)\n",
    "    if \"Score_1\" in scored.columns: return scored[\"Score_1\"]\n",
    "    if \"Score\" in scored.columns and \"prediction_label\" in scored.columns:\n",
    "        return pd.Series(np.where(scored[\"prediction_label\"]==1, scored[\"Score\"], 1.0-scored[\"Score\"]), index=Xdf.index)\n",
    "    if \"prediction_score\" in scored.columns and \"prediction_label\" in scored.columns:\n",
    "        return pd.Series(np.where(scored[\"prediction_label\"]==1, scored[\"prediction_score\"], 1.0-scored[\"prediction_score\"]), index=Xdf.index)\n",
    "\n",
    "    raise AttributeError(\"No hay forma de obtener probas del VotingClassifier (hard). Considera reentrenar con voting='soft'.\")\n",
    "\n",
    "def recomendar_top3_cls(user: dict) -> pd.DataFrame:\n",
    "    X0 = _build_candidates(user)\n",
    "    X  = _prepare_for_pipeline(X0)                 # << quita id_usuario y ajusta tipos\n",
    "\n",
    "    # Intento directo (por si NO es 'hard'); si falla, uso agregación de base estimators\n",
    "    try:\n",
    "        proba = CLS.predict_proba(X)\n",
    "        classes = getattr(CLS, \"classes_\", None)\n",
    "        if classes is not None and (1 in list(classes) or \"1\" in list(classes)):\n",
    "            pos = list(classes).index(1) if 1 in list(classes) else list(classes).index(\"1\")\n",
    "        else:\n",
    "            pos = proba.shape[1]-1\n",
    "        prob_like = pd.Series(proba[:, pos], index=X.index)\n",
    "    except Exception:\n",
    "        prob_like = _prob_like_from_hard_voting(CLS, X)\n",
    "\n",
    "    scored = X0.assign(prob_like=prob_like.values)  # unimos a los campos legibles (tipo_sitio, etc.)\n",
    "\n",
    "    cols = [\"nombre_sitio\",\"tipo_sitio\",\"costo_entrada\",\"accesibilidad_general\",\n",
    "            \"afinidad_tipo\",\"ratio_costo_presu\",\"prob_like\"]\n",
    "    cols = [c for c in cols if c in scored.columns]\n",
    "    return scored[cols].nlargest(3, \"prob_like\").reset_index(drop=True)\n",
    "\n",
    "def recomendar_top3_reg(user: dict) -> pd.DataFrame:\n",
    "    X0 = _build_candidates(user)\n",
    "    X  = _prepare_for_pipeline(X0)                 # opcional (predict_model ya lo ignora)\n",
    "    scored = predict_reg(REG, data=X)\n",
    "    pred_col = \"prediction_label\" if \"prediction_label\" in scored.columns else (\"Label\" if \"Label\" in scored.columns else None)\n",
    "    if pred_col is None:\n",
    "        raise KeyError(f\"No encuentro columna de predicción en regresión. Tengo: {list(scored.columns)[:20]}\")\n",
    "    scored = scored.rename(columns={pred_col: \"rating_prev\"})\n",
    "\n",
    "    cols = [\"nombre_sitio\",\"tipo_sitio\",\"costo_entrada\",\"accesibilidad_general\",\n",
    "            \"afinidad_tipo\",\"ratio_costo_presu\",\"rating_prev\"]\n",
    "    cols = [c for c in cols if c in X0.columns] + [\"rating_prev\"]\n",
    "    out = X0.join(scored[[\"rating_prev\"]]).nlargest(3, \"rating_prev\")[cols].reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "usuario_demo = {\n",
    "    \"id_usuario\":\"U_demo\",\"edad\":50,\"nacionalidad\":\"Colombia\",\"origen\":\"Bogotá\",\n",
    "    \"tipo_turista_preferido\":\"cultural\",\"compañia_viaje\":\"pareja\",\n",
    "    \"frecuencia_viaje\":2,\"restricciones_movilidad\":\"ninguna\",\n",
    "    \"presupuesto_estimado\":230000,\"sitios_visitados\":6,\n",
    "    \"calificacion_sitios_previos\":4.3,\"tiempo_estancia_promedio\":90,\n",
    "    \"epoca_visita\":\"fin_de_semana_puente\"\n",
    "}\n",
    "\n",
    "print(\"Top-3 (clasificación):\")\n",
    "print(recomendar_top3_cls(usuario_demo).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop-3 (regresión):\")\n",
    "print(recomendar_top3_reg(usuario_demo).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63622ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificación\n",
    "clf = final_cls.named_steps[\"trained_model\"]\n",
    "print(clf)               # tipo (VotingClassifier)\n",
    "print([type(e).__name__ for e in clf.estimators_])  # modelos base\n",
    "\n",
    "# Regresión\n",
    "reg = final_reg.named_steps[\"trained_model\"]\n",
    "print(reg)               # tipo (VotingRegressor)\n",
    "print([type(e).__name__ for e in reg.estimators_])  # modelos base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71309f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor\n",
    "\n",
    "def inspect_pycaret_pipeline(pipe):\n",
    "    # 1) nombres de pasos\n",
    "    step_names = [name for name, _ in pipe.steps]\n",
    "    print(\"Pasos del Pipeline:\", step_names)\n",
    "\n",
    "    # 2) último paso = estimador final\n",
    "    last_name, last_step = pipe.steps[-1]\n",
    "    print(f\"Paso final: {last_name}  →  {type(last_step).__name__}\")\n",
    "\n",
    "    # 3) según el tipo, muestra su composición\n",
    "    if isinstance(last_step, (VotingClassifier, VotingRegressor)):\n",
    "        # estimadores base (usamos estimators_ si está disponible; si no, estimators)\n",
    "        base = getattr(last_step, \"estimators_\", None)\n",
    "        if base is None:\n",
    "            base = [est for _, est in getattr(last_step, \"estimators\", [])]\n",
    "        print(\"Tipo Voting:\", getattr(last_step, \"voting\", \"—\"))\n",
    "        print(\"Estimadores base:\", [type(e).__name__ for e in base])\n",
    "\n",
    "    elif isinstance(last_step, (StackingClassifier, StackingRegressor)):\n",
    "        base = getattr(last_step, \"estimators_\", None)\n",
    "        if base is None:\n",
    "            base = [est for _, est in getattr(last_step, \"estimators\", [])]\n",
    "        meta = getattr(last_step, \"final_estimator_\", getattr(last_step, \"final_estimator\", None))\n",
    "        print(\"Estimadores base (stacking):\", [type(e).__name__ for e in base])\n",
    "        print(\"Meta-modelo:\", type(meta).__name__ if meta is not None else \"—\")\n",
    "\n",
    "    else:\n",
    "        print(\"Modelo único (no es ensamble).\")\n",
    "\n",
    "# Inspecciona tus modelos guardados:\n",
    "inspect_pycaret_pipeline(CLS)   # clasificación\n",
    "inspect_pycaret_pipeline(REG)   # regresión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, est_final = CLS.steps[-1]\n",
    "print(list(est_final.get_params().keys())[:20])  # algunas claves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model, blend_models,\n",
    "    calibrate_model, finalize_model, save_model, pull\n",
    ")\n",
    "\n",
    "# Usa el mismo train_df y el MISMO setup que ya te funcionó (groupkfold + ignore id_usuario)\n",
    "setup(\n",
    "    data=train_df[[\"id_usuario\"] + CAT_COLS_X + NUM_COLS_X + [\"y_like\"]],\n",
    "    target=\"y_like\",\n",
    "    session_id=RANDOM_SEED,\n",
    "    fold=5, fold_strategy=\"groupkfold\", fold_groups=\"id_usuario\",\n",
    "    categorical_features=CAT_COLS_X,\n",
    "    ignore_features=[\"id_usuario\"],\n",
    "    remove_multicollinearity=True, multicollinearity_threshold=0.95,\n",
    "    imputation_type=\"simple\",\n",
    "    fix_imbalance=False,   # activa luego si lo necesitas\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# candidatos y tuning\n",
    "cands = compare_models(n_select=5, sort=\"AUC\")\n",
    "tuned = [tune_model(m, optimize=\"AUC\") for m in cands]\n",
    "# quédate con modelos que soportan predict_proba (requisito para 'soft')\n",
    "tuned = [m for m in tuned if hasattr(m, \"predict_proba\")]\n",
    "\n",
    "# ensamble 'soft' (promedia probabilidades)\n",
    "soft = blend_models(estimator_list=tuned, method=\"soft\", choose_better=True)\n",
    "\n",
    "# (opcional pero recomendado) calibra probabilidades\n",
    "soft_cal = calibrate_model(soft, method=\"isotonic\")\n",
    "\n",
    "# cierra y guarda\n",
    "final_soft = finalize_model(soft_cal)\n",
    "save_model(final_soft, \"modelo_cls_like_soft_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model\n",
    "CLS_SOFT = load_model(\"modelo_cls_like_soft_v1\")\n",
    "\n",
    "def recomendar_top3_cls_soft(user: dict) -> pd.DataFrame:\n",
    "    X0 = _build_candidates(user)\n",
    "    X  = _prepare_for_pipeline(X0)          # quita id_usuario y asegura tipos\n",
    "    proba = CLS_SOFT.predict_proba(X)       # ahora sí disponible\n",
    "    pos = list(CLS_SOFT.classes_).index(1) if 1 in CLS_SOFT.classes_ else proba.shape[1]-1\n",
    "    X0[\"prob_like\"] = proba[:, pos]\n",
    "    cols = [\"nombre_sitio\",\"tipo_sitio\",\"costo_entrada\",\"accesibilidad_general\",\n",
    "            \"afinidad_tipo\",\"ratio_costo_presu\",\"prob_like\"]\n",
    "    return X0[cols].nlargest(3, \"prob_like\").reset_index(drop=True)\n",
    "\n",
    "# probar:\n",
    "print(recomendar_top3_cls_soft(usuario_demo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "villaIA_leyva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
